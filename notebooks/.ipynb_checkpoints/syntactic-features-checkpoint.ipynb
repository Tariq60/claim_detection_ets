{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import networkx as nx\n",
    "model_dir = '/Users/talhindi/miniconda3/lib/python3.7/site-packages/en_core_web_sm/en_core_web_sm-2.1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = pd.read_csv('../data/SG2017/train-test-split.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_txt_prg_list = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017/*.txt\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_txt_prg_list.append(essay)\n",
    "\n",
    "essay_txt_str = []\n",
    "for essay in essays_txt_prg_list:\n",
    "    essay_txt_str.append(''.join(essay))\n",
    "#     essay_str = ''\n",
    "#     for prg in essay:\n",
    "#         essay_str += prg.rstrip()+' '\n",
    "#     essay_txt_str.append(essay_str)\n",
    "\n",
    "    \n",
    "essays_ann = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017/*.ann\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_ann.append(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_segments = []\n",
    "\n",
    "for essay in essays_ann:    \n",
    "    segments = []\n",
    "    \n",
    "    for line in essay:\n",
    "        if line[0] == 'T':\n",
    "            _, label_s_e, text = line.rstrip().split('\\t')\n",
    "            label, start, end = label_s_e.split()\n",
    "            segments.append((label, int(start), int(end), text))\n",
    "            \n",
    "    segments.sort(key = lambda element : element[1])\n",
    "    essays_segments.append(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_spacy(essay_spacy, segments):\n",
    "    '''O = 0, Arg-B = 1, Arg-I = 2'''\n",
    "    \n",
    "    doc_len = len(essay_spacy)\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "    arg_seg_starts = [start for arg_type, start, end, text in segments]\n",
    "    \n",
    "    for token in essay_spacy:\n",
    "        arg_I_token = False\n",
    "\n",
    "        if token.idx in arg_seg_starts:\n",
    "#                 labels.append('B')\n",
    "            labels.append(1.0)\n",
    "            tokens.append(token.text)\n",
    "            assert token.text in segments[arg_seg_starts.index(token.idx)][-1]\n",
    "        else:\n",
    "            for _, start, end, _ in segments:\n",
    "                if token.idx > start and token.idx+len(token) <= end:\n",
    "#                         labels.append('I')\n",
    "                    labels.append(2.0)\n",
    "                    tokens.append(token.text)\n",
    "                    arg_I_token = True\n",
    "            if not arg_I_token:\n",
    "#                     labels.append('O')\n",
    "                labels.append(0.0)\n",
    "                tokens.append(token.text)\n",
    "\n",
    "    assert len(labels) == doc_len\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int, {0.0: 39617, 1.0: 4823, 2.0: 75312}),\n",
       " defaultdict(int, {0.0: 9801, 1.0: 1266, 2.0: 18748}))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with new lines\n",
    "train_BIO,test_BIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(model_dir)\n",
    "\n",
    "essay_spacy = []\n",
    "for essay in essay_txt_str:\n",
    "    essay_spacy.append(nlp(essay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int, {0.0: 39617, 1.0: 4823, 2.0: 75312}),\n",
       " defaultdict(int, {0.0: 9801, 1.0: 1266, 2.0: 18748}))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting labels from each type\n",
    "# without new lines\n",
    "token_labels = []\n",
    "train_BIO = defaultdict(int)\n",
    "test_BIO = defaultdict(int)\n",
    "\n",
    "for doc, segments, group in zip(essay_spacy, essays_segments, train_test_split.SET):\n",
    "    tokens, labels = get_labels_spacy(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        for label in  labels:\n",
    "            train_BIO[label] += 1\n",
    "    else:\n",
    "        for label in  labels:\n",
    "            test_BIO[label] += 1\n",
    "    \n",
    "train_BIO,test_BIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(essay_spacy):\n",
    "    '''Part-of-speech: The tokenâ€™s part-of-speech'''\n",
    "    pos_features = []\n",
    "    for token in essay_spacy:\n",
    "        pos_features.append({'pos_{}'.format(token.pos_): 1.0})\n",
    "    \n",
    "    return pos_features\n",
    "            \n",
    "    \n",
    "    \n",
    "def get_lca_features_sent(sent, get_average, get_types=False):\n",
    "    '''Lowest common ancestor (LCA):\n",
    "        Normalized length of the path to the LCA with the *following* and *preceding* token in the parse tree'''    \n",
    "    edges = []\n",
    "    for token in sent:\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}-{1}'.format(token.text, token.i),'{0}-{1}'.format(child.text, child.i)))\n",
    "            \n",
    "    graph = nx.Graph(edges)\n",
    "    lca_matrix = sent.get_lca_matrix()\n",
    "#     print(graph, edges, lca_matrix)\n",
    "    \n",
    "    lca_prev_next_path_sent, lca_types_sent = [], []\n",
    "    for token_id, token in enumerate(sent):\n",
    "        if token_id == 0:\n",
    "            token_prev_lca = -1\n",
    "            token_next_lca = lca_matrix[token_id, token_id+1]\n",
    "        elif token_id == len(sent)-1:\n",
    "            token_prev_lca = lca_matrix[token_id, token_id-1]\n",
    "            token_next_lca = -1\n",
    "        else:\n",
    "            token_prev_lca = lca_matrix[token_id, token_id-1]\n",
    "            token_next_lca = lca_matrix[token_id, token_id+1]\n",
    "        \n",
    "        # adding index to tokens to retrieve node in graph. node-name = token-index\n",
    "        source_token = '{0}-{1}'.format(token.text, token.i)\n",
    "        lca_types_token = {}\n",
    "        \n",
    "        # token, previous_token shortest path to lca\n",
    "        if token_prev_lca != -1:\n",
    "            source_prev_token = '{0}-{1}'.format(sent[token_id-1].text, sent[token_id-1].i)\n",
    "            target_token_prev_lca = '{0}-{1}'.format(sent[token_prev_lca].text, sent[token_prev_lca].i)\n",
    "#             lca_types_token['lca_prev_{}'.format(sent[token_prev_lca].pos_)] = 1.0\n",
    "            lca_types_token['lca_prev_{}'.format(sent[token_prev_lca].dep_)] = 1.0\n",
    "            \n",
    "            lca_prev_path_token = nx.shortest_path_length(graph, source=source_token, target=target_token_prev_lca)\n",
    "            lca_prev_path_prev = nx.shortest_path_length(graph, source=source_prev_token, target=target_token_prev_lca)\n",
    "            \n",
    "            if get_average:\n",
    "                lca_prev_path = np.mean((lca_prev_path_token, lca_prev_path_prev))\n",
    "            else:\n",
    "                lca_prev_path = lca_prev_path_token\n",
    "        else:\n",
    "            lca_prev_path = -1\n",
    "            \n",
    "        # token, next_token shortest path to lca\n",
    "        if token_next_lca != -1:\n",
    "            source_next_token = '{0}-{1}'.format(sent[token_id+1].text, sent[token_id+1].i)\n",
    "            target_token_next_lca = '{0}-{1}'.format(sent[token_next_lca].text, sent[token_next_lca].i)\n",
    "#             lca_types_token['lca_next_{}'.format(sent[token_next_lca].pos_)] = 1.0\n",
    "            lca_types_token['lca_next_{}'.format(sent[token_next_lca].dep_)] = 1.0\n",
    "            \n",
    "            lca_next_path_token = nx.shortest_path_length(graph, source=source_token, target=target_token_next_lca)\n",
    "            lca_next_path_next = nx.shortest_path_length(graph, source=source_next_token, target=target_token_next_lca)\n",
    "            \n",
    "            if get_average:\n",
    "                lca_next_path = np.mean((lca_next_path_token, lca_next_path_next))\n",
    "            else:\n",
    "                lca_next_path = lca_next_path_token\n",
    "        else:\n",
    "            lca_next_path = -1\n",
    "        \n",
    "        # adding LCA features of this token\n",
    "        if get_average:\n",
    "            lca_prev_next_path_sent.append({'lca_prev_path_avg': lca_prev_path, 'lca_next_path_avg': lca_next_path})\n",
    "        else:\n",
    "            lca_prev_next_path_sent.append({'lca_prev_path': lca_prev_path, 'lca_next_path': lca_next_path})\n",
    "        lca_types_sent.append(lca_types_token)\n",
    "     \n",
    "    # returning LCA features for all tokens in the sentence\n",
    "    if not get_types:\n",
    "        return lca_prev_next_path_sent\n",
    "    else:\n",
    "        return lca_types_sent\n",
    "    \n",
    "\n",
    "def get_lca_features_doc(doc, get_average=True):\n",
    "    token_lca, sent_id = [], 0\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "#         print(sent_id)\n",
    "        if len(sent) > 1:\n",
    "            sent_lca = get_lca_features_sent(sent, get_average, False)\n",
    "        else:\n",
    "            assert len(sent) == 1\n",
    "            sent_lca = [{'lca_prev_path': 0, 'lca_next_path': 0}]\n",
    "        \n",
    "        for feature in sent_lca:\n",
    "            token_lca.append(feature)\n",
    "        sent_id += 1\n",
    "    \n",
    "    return token_lca\n",
    "\n",
    "\n",
    "def get_lca_types_doc(doc):\n",
    "    '''LCA types: The two constituent types of the LCA of the current token and its preceding and following token'''\n",
    "    token_lca, sent_id = [], 0\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "#         print(sent_id)\n",
    "        if len(sent) > 1:\n",
    "            sent_lca = get_lca_features_sent(sent, False, True)\n",
    "        else:\n",
    "            assert len(sent) == 1\n",
    "            sent_lca = [{}]\n",
    "        \n",
    "        for feature in sent_lca:\n",
    "            token_lca.append(feature)\n",
    "        sent_id += 1\n",
    "    \n",
    "    return token_lca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tags\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_pos_spacy.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_pos_spacy.jsonlines', 'w')\n",
    "\n",
    "for doc, segments, group in zip(essay_spacy, essays_segments, train_test_split.SET):\n",
    "    \n",
    "    features = get_pos(doc)\n",
    "    tokens, labels = get_labels_spacy(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_pos_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_pos_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCA \n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_LCA_spacy.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_LCA_spacy.jsonlines', 'w')\n",
    "\n",
    "for i, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "#     print(i)\n",
    "    features = get_lca_features_doc(doc, False)\n",
    "    tokens, labels = get_labels_spacy(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_LCA_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_LCA_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCA avg\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_LCA_avg_spacy.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_LCA_avg_spacy.jsonlines', 'w')\n",
    "\n",
    "for i, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "#     print(i)\n",
    "    features = get_lca_features_doc(doc)\n",
    "    tokens, labels = get_labels_spacy(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_LCA_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_LCA_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCA type\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_LCA_type_spacy.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_LCA_type_spacy.jsonlines', 'w')\n",
    "\n",
    "for i, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "#     print(i)\n",
    "    features = get_lca_types_doc(doc)\n",
    "    tokens, labels = get_labels_spacy(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_LCA_type_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_LCA_type_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCA type -- old == pos\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_LCA_pos_spacy.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_LCA_pos_spacy.jsonlines', 'w')\n",
    "\n",
    "for i, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "#     print(i)\n",
    "    features = get_lca_types_doc(doc)\n",
    "    tokens, labels = get_labels_spacy(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_LCA_pos_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_LCA_pos_spacy.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['convulsions', 'caused', 'by', 'fever']\n"
     ]
    }
   ],
   "source": [
    "# Load spacy's dependency tree into a networkx graph\n",
    "edges = []\n",
    "for token in doc:\n",
    "    for child in token.children:\n",
    "        edges.append(('{0}'.format(token.lower_),'{0}'.format(child.lower_)))\n",
    "\n",
    "\n",
    "graph = nx.Graph(edges)\n",
    "\n",
    "# Get the length and path\n",
    "entity1 = 'Convulsions'.lower()\n",
    "entity2 = 'fever'\n",
    "\n",
    "print(nx.shortest_path_length(graph, source=entity1, target=entity2))\n",
    "print(nx.shortest_path(graph, source=entity1, target=entity2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdp_path(doc, subj, obj, lca_matrix):\n",
    "    lca = lca_matrix[subj, obj]\n",
    "  \n",
    "    current_node = doc[subj]\n",
    "    subj_path = [current_node]\n",
    "    if lca != -1: \n",
    "        if lca != subj: \n",
    "            while current_node.head.i != lca:\n",
    "                current_node = current_node.head\n",
    "                subj_path.append(current_node)\n",
    "            subj_path.append(current_node.head)\n",
    "            \n",
    "    current_node = doc[obj]\n",
    "    obj_path = [current_node]\n",
    "    if lca != -1: \n",
    "        if lca != obj: \n",
    "            while current_node.head.i != lca:\n",
    "                current_node = current_node.head\n",
    "                obj_path.append(current_node)\n",
    "            obj_path.append(current_node.head)\n",
    "  \n",
    "    return subj_path + obj_path[::-1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('caused', 'Convulsions', 'nsubjpass')\n",
      "('occur', 'that', 'nsubj')\n",
      "('Convulsions', 'occur', 'relcl')\n",
      "('occur', 'after', 'prep')\n",
      "('caused', 'DTaP', 'nsubjpass')\n",
      "('caused', 'are', 'auxpass')\n",
      "('caused', 'caused', 'ROOT')\n",
      "('caused', 'by', 'agent')\n",
      "('fever', 'a', 'det')\n",
      "('by', 'fever', 'pobj')\n",
      "('caused', '.', 'punct')\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(model_dir)\n",
    "doc = nlp(u'Convulsions that occur after DTaP are caused by a fever.')\n",
    "\n",
    "for token in doc:\n",
    "    print((token.head.text, token.text, token.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  6,  6,  6,  6,  6,  6,  6],\n",
       "       [ 0,  1,  2,  2,  6,  6,  6,  6,  6,  6,  6],\n",
       "       [ 0,  2,  2,  2,  6,  6,  6,  6,  6,  6,  6],\n",
       "       [ 0,  2,  2,  3,  6,  6,  6,  6,  6,  6,  6],\n",
       "       [ 6,  6,  6,  6,  4,  6,  6,  6,  6,  6,  6],\n",
       "       [ 6,  6,  6,  6,  6,  5,  6,  6,  6,  6,  6],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  6],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6,  7,  8,  9,  6],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6,  7,  9,  9,  6],\n",
       "       [ 6,  6,  6,  6,  6,  6,  6,  6,  6,  6, 10]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.get_lca_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall/NNP <--compound-- Street/NNP\n",
      "Street/NNP <--compound-- Journal/NNP\n",
      "Journal/NNP <--nsubj-- published/VBD\n",
      "just/RB <--advmod-- published/VBD\n",
      "published/VBD <--ROOT-- published/VBD\n",
      "an/DT <--det-- piece/NN\n",
      "interesting/JJ <--amod-- piece/NN\n",
      "piece/NN <--dobj-- published/VBD\n",
      "on/IN <--prep-- piece/NN\n",
      "crypto/NN <--amod-- currencies/NNS\n",
      "currencies/NNS <--pobj-- on/IN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(essay_txt_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers.,\n",
       " continue,\n",
       " continue)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents = list(doc.sents)\n",
    "sents[2], sents[2][0], sents[2][0].head\n",
    "sents[2], sents[2][9], sents[2][9].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should/MD <--aux-- taught/VBN\n",
      "students/NNS <--nsubjpass-- taught/VBN\n",
      "be/VB <--auxpass-- taught/VBN\n",
      "taught/VBN <--ROOT-- taught/VBN\n",
      "to/TO <--aux-- compete/VB\n",
      "compete/VB <--xcomp-- taught/VBN\n",
      "or/CC <--cc-- compete/VB\n",
      "to/TO <--aux-- cooperate/VB\n",
      "cooperate/VB <--conj-- compete/VB\n",
      "?/. <--punct-- taught/VBN\n",
      "\n",
      "\n",
      "/_SP <---- ?/.\n",
      "\n",
      "It/PRP <--nsubjpass-- said/VBN\n",
      "is/VBZ <--auxpass-- said/VBN\n",
      "always/RB <--advmod-- said/VBN\n",
      "said/VBN <--ROOT-- said/VBN\n",
      "that/IN <--mark-- promote/VB\n",
      "competition/NN <--nsubj-- promote/VB\n",
      "can/MD <--aux-- promote/VB\n",
      "effectively/RB <--advmod-- promote/VB\n",
      "promote/VB <--ccomp-- said/VBN\n",
      "the/DT <--det-- development/NN\n",
      "development/NN <--dobj-- promote/VB\n",
      "of/IN <--prep-- development/NN\n",
      "economy/NN <--pobj-- of/IN\n",
      "./. <--punct-- said/VBN\n",
      "\n",
      "In/IN <--prep-- continue/VBP\n",
      "order/NN <--pobj-- In/IN\n",
      "to/TO <--aux-- survive/VB\n",
      "survive/VB <--acl-- order/NN\n",
      "in/IN <--prep-- survive/VB\n",
      "the/DT <--det-- competition/NN\n",
      "competition/NN <--pobj-- in/IN\n",
      ",/, <--punct-- continue/VBP\n",
      "companies/NNS <--nsubj-- continue/VBP\n",
      "continue/VBP <--ROOT-- continue/VBP\n",
      "to/TO <--aux-- improve/VB\n",
      "improve/VB <--xcomp-- continue/VBP\n",
      "their/PRP$ <--poss-- products/NNS\n",
      "products/NNS <--dobj-- improve/VB\n",
      "and/CC <--cc-- products/NNS\n",
      "service/NN <--conj-- products/NNS\n",
      ",/, <--punct-- continue/VBP\n",
      "and/CC <--cc-- continue/VBP\n",
      "as/IN <--conj-- continue/VBP\n",
      "a/DT <--det-- result/NN\n",
      "result/NN <--pobj-- as/IN\n",
      ",/, <--punct-- as/IN\n",
      "the/DT <--det-- prospers/NNS\n",
      "whole/JJ <--amod-- prospers/NNS\n",
      "society/NN <--compound-- prospers/NNS\n",
      "prospers/NNS <--pobj-- as/IN\n",
      "./. <--punct-- continue/VBP\n",
      "\n",
      "However/RB <--advmod-- is/VBZ\n",
      ",/, <--punct-- is/VBZ\n",
      "when/WRB <--advmod-- discuss/VBP\n",
      "we/PRP <--nsubj-- discuss/VBP\n",
      "discuss/VBP <--advcl-- concerned/JJ\n",
      "the/DT <--det-- issue/NN\n",
      "issue/NN <--dobj-- discuss/VBP\n",
      "of/IN <--prep-- issue/NN\n",
      "competition/NN <--pobj-- of/IN\n",
      "or/CC <--cc-- competition/NN\n",
      "cooperation/NN <--conj-- competition/NN\n",
      ",/, <--punct-- concerned/JJ\n",
      "what/WP <--pobj-- are/VBP\n",
      "we/PRP <--nsubj-- are/VBP\n",
      "are/VBP <--auxpass-- concerned/JJ\n",
      "concerned/JJ <--csubj-- is/VBZ\n",
      "about/IN <--prep-- concerned/JJ\n",
      "is/VBZ <--ROOT-- is/VBZ\n",
      "not/RB <--neg-- is/VBZ\n",
      "the/DT <--det-- society/NN\n",
      "whole/JJ <--amod-- society/NN\n",
      "society/NN <--attr-- is/VBZ\n",
      ",/, <--punct-- is/VBZ\n",
      "but/CC <--cc-- is/VBZ\n",
      "the/DT <--det-- development/NN\n",
      "development/NN <--conj-- is/VBZ\n",
      "of/IN <--prep-- development/NN\n",
      "an/DT <--det-- individual/NN\n",
      "individual/NN <--poss-- life/NN\n",
      "'s/POS <--case-- individual/NN\n",
      "whole/JJ <--amod-- life/NN\n",
      "life/NN <--pobj-- of/IN\n",
      "./. <--punct-- is/VBZ\n",
      "\n",
      "From/IN <--prep-- believe/VBP\n",
      "this/DT <--det-- point/NN\n",
      "point/NN <--pobj-- From/IN\n",
      "of/IN <--prep-- point/NN\n",
      "view/NN <--pobj-- of/IN\n",
      ",/, <--punct-- believe/VBP\n",
      "I/PRP <--nsubj-- believe/VBP\n",
      "firmly/RB <--advmod-- believe/VBP\n",
      "believe/VBP <--ROOT-- believe/VBP\n",
      "that/IN <--mark-- attach/VB\n",
      "we/PRP <--nsubj-- attach/VB\n",
      "should/MD <--aux-- attach/VB\n",
      "attach/VB <--ccomp-- believe/VBP\n",
      "more/JJR <--amod-- importance/NN\n",
      "importance/NN <--dobj-- attach/VB\n",
      "to/IN <--aux-- cooperation/NN\n",
      "cooperation/NN <--relcl-- importance/NN\n",
      "during/IN <--prep-- cooperation/NN\n",
      "primary/JJ <--amod-- education/NN\n",
      "education/NN <--pobj-- during/IN\n",
      "./. <--punct-- believe/VBP\n",
      "\n",
      "/_SP <---- ./.\n",
      "\n",
      "First/RB <--advmod-- learn/VB\n",
      "of/IN <--prep-- First/RB\n",
      "all/DT <--pobj-- of/IN\n",
      ",/, <--punct-- learn/VB\n",
      "through/IN <--prep-- learn/VB\n",
      "cooperation/NN <--pobj-- through/IN\n",
      ",/, <--punct-- learn/VB\n",
      "children/NNS <--nsubj-- learn/VB\n",
      "can/MD <--aux-- learn/VB\n",
      "learn/VB <--ROOT-- learn/VB\n",
      "about/IN <--prep-- learn/VB\n",
      "interpersonal/JJ <--amod-- skills/NNS\n",
      "skills/NNS <--pobj-- about/IN\n",
      "which/WDT <--nsubj-- are/VBP\n",
      "are/VBP <--relcl-- skills/NNS\n",
      "significant/JJ <--acomp-- are/VBP\n",
      "in/IN <--prep-- significant/JJ\n",
      "the/DT <--det-- life/NN\n",
      "future/JJ <--amod-- life/NN\n",
      "life/NN <--pobj-- in/IN\n",
      "of/IN <--prep-- life/NN\n",
      "all/DT <--det-- students/NNS\n",
      "students/NNS <--pobj-- of/IN\n",
      "./. <--punct-- learn/VB\n",
      "\n",
      "What/WP <--dobj-- acquired/VBD\n",
      "we/PRP <--nsubj-- acquired/VBD\n",
      "acquired/VBD <--csubj-- is/VBZ\n",
      "from/IN <--prep-- acquired/VBD\n",
      "team/NN <--compound-- work/NN\n",
      "work/NN <--pobj-- from/IN\n",
      "is/VBZ <--ROOT-- is/VBZ\n",
      "not/RB <--neg-- is/VBZ\n",
      "only/RB <--advmod-- not/RB\n",
      "how/WRB <--advmod-- achieve/VB\n",
      "to/TO <--aux-- achieve/VB\n",
      "achieve/VB <--xcomp-- is/VBZ\n",
      "the/DT <--det-- goal/NN\n",
      "same/JJ <--amod-- goal/NN\n",
      "goal/NN <--dobj-- achieve/VB\n",
      "with/IN <--prep-- goal/NN\n",
      "others/NNS <--pobj-- with/IN\n",
      "but/CC <--cc-- others/NNS\n",
      "more/RBR <--advmod-- importantly/RB\n",
      "importantly/RB <--advmod-- achieve/VB\n",
      ",/, <--punct-- achieve/VB\n",
      "how/WRB <--advmod-- get/VB\n",
      "to/TO <--aux-- get/VB\n",
      "get/VB <--dep-- achieve/VB\n",
      "along/RP <--prt-- get/VB\n",
      "with/IN <--prep-- get/VB\n",
      "others/NNS <--pobj-- with/IN\n",
      "./. <--punct-- is/VBZ\n",
      "\n",
      "During/IN <--prep-- learn/VB\n",
      "the/DT <--det-- process/NN\n",
      "process/NN <--pobj-- During/IN\n",
      "of/IN <--prep-- process/NN\n",
      "cooperation/NN <--pobj-- of/IN\n",
      ",/, <--punct-- learn/VB\n",
      "children/NNS <--nsubj-- learn/VB\n",
      "can/MD <--aux-- learn/VB\n",
      "learn/VB <--ROOT-- learn/VB\n",
      "about/IN <--prep-- learn/VB\n",
      "how/WRB <--advmod-- listen/VB\n",
      "to/TO <--aux-- listen/VB\n",
      "listen/VB <--pcomp-- about/IN\n",
      "to/IN <--prep-- listen/VB\n",
      "opinions/NNS <--pobj-- to/IN\n",
      "of/IN <--prep-- opinions/NNS\n",
      "others/NNS <--pobj-- of/IN\n",
      ",/, <--punct-- listen/VB\n",
      "how/WRB <--advmod-- communicate/VB\n",
      "to/TO <--aux-- communicate/VB\n",
      "communicate/VB <--conj-- listen/VB\n",
      "with/IN <--prep-- communicate/VB\n",
      "others/NNS <--pobj-- with/IN\n",
      ",/, <--punct-- communicate/VB\n",
      "how/WRB <--advmod-- think/VB\n",
      "to/TO <--aux-- think/VB\n",
      "think/VB <--advcl-- communicate/VB\n",
      "comprehensively/RB <--advmod-- think/VB\n",
      ",/, <--punct-- think/VB\n",
      "and/CC <--cc-- think/VB\n",
      "even/RB <--advmod-- how/WRB\n",
      "how/WRB <--advmod-- compromise/VB\n",
      "to/TO <--aux-- compromise/VB\n",
      "compromise/VB <--conj-- think/VB\n",
      "with/IN <--prep-- compromise/VB\n",
      "other/JJ <--amod-- members/NNS\n",
      "team/NN <--compound-- members/NNS\n",
      "members/NNS <--pobj-- with/IN\n",
      "when/WRB <--advmod-- occurred/VBD\n",
      "conflicts/NNS <--nsubj-- occurred/VBD\n",
      "occurred/VBD <--advcl-- compromise/VB\n",
      "./. <--punct-- learn/VB\n",
      "\n",
      "All/DT <--nsubj-- help/VBP\n",
      "of/IN <--prep-- All/DT\n",
      "these/DT <--det-- skills/NNS\n",
      "skills/NNS <--pobj-- of/IN\n",
      "help/VBP <--ROOT-- help/VBP\n",
      "them/PRP <--nsubj-- get/VB\n",
      "to/TO <--aux-- get/VB\n",
      "get/VB <--ccomp-- help/VBP\n",
      "on/RP <--prt-- get/VB\n",
      "well/RB <--advmod-- get/VB\n",
      "with/IN <--prep-- get/VB\n",
      "other/JJ <--amod-- people/NNS\n",
      "people/NNS <--pobj-- with/IN\n",
      "and/CC <--cc-- help/VBP\n",
      "will/MD <--aux-- benefit/VB\n",
      "benefit/VB <--conj-- help/VBP\n",
      "them/PRP <--dobj-- benefit/VB\n",
      "for/IN <--prep-- benefit/VB\n",
      "the/DT <--det-- life/NN\n",
      "whole/JJ <--amod-- life/NN\n",
      "life/NN <--pobj-- for/IN\n",
      "./. <--punct-- help/VBP\n",
      "\n",
      "/_SP <---- ./.\n",
      "\n",
      "On/IN <--prep-- is/VBZ\n",
      "the/DT <--det-- hand/NN\n",
      "other/JJ <--amod-- hand/NN\n",
      "hand/NN <--pobj-- On/IN\n",
      ",/, <--punct-- is/VBZ\n",
      "the/DT <--det-- significance/NN\n",
      "significance/NN <--nsubj-- is/VBZ\n",
      "of/IN <--prep-- significance/NN\n",
      "competition/NN <--pobj-- of/IN\n",
      "is/VBZ <--ROOT-- is/VBZ\n",
      "that/IN <--nsubj-- is/VBZ\n",
      "how/WRB <--advmod-- become/VB\n",
      "to/TO <--aux-- become/VB\n",
      "become/VB <--xcomp-- is/VBZ\n",
      "more/JJR <--amod-- excellence/NN\n",
      "excellence/NN <--attr-- become/VB\n",
      "to/TO <--aux-- gain/VB\n",
      "gain/VB <--acl-- excellence/NN\n",
      "the/DT <--det-- victory/NN\n",
      "victory/NN <--dobj-- gain/VB\n",
      "./. <--punct-- is/VBZ\n",
      "\n",
      "Hence/RB <--advmod-- said/VBN\n",
      "it/PRP <--nsubjpass-- said/VBN\n",
      "is/VBZ <--auxpass-- said/VBN\n",
      "always/RB <--advmod-- said/VBN\n",
      "said/VBN <--ROOT-- said/VBN\n",
      "that/IN <--mark-- makes/VBZ\n",
      "competition/NN <--nsubj-- makes/VBZ\n",
      "makes/VBZ <--ccomp-- said/VBN\n",
      "the/DT <--det-- society/NN\n",
      "society/NN <--nsubj-- effective/JJ\n",
      "more/RBR <--advmod-- effective/JJ\n",
      "effective/JJ <--ccomp-- makes/VBZ\n",
      "./. <--punct-- said/VBN\n",
      "\n",
      "However/RB <--ROOT-- However/RB\n",
      ",/, <--punct-- However/RB\n",
      "when/WRB <--advmod-- consider/VBP\n",
      "we/PRP <--nsubj-- consider/VBP\n",
      "consider/VBP <--advcl-- However/RB\n",
      "about/IN <--prep-- consider/VBP\n",
      "the/DT <--det-- question/NN\n",
      "question/NN <--pobj-- about/IN\n",
      "that/IN <--mark-- find/VBP\n",
      "how/WRB <--advmod-- win/VB\n",
      "to/TO <--aux-- win/VB\n",
      "win/VB <--advcl-- find/VBP\n",
      "the/DT <--det-- game/NN\n",
      "game/NN <--dobj-- win/VB\n",
      ",/, <--punct-- find/VBP\n",
      "we/PRP <--nsubj-- find/VBP\n",
      "always/RB <--advmod-- find/VBP\n",
      "find/VBP <--acl-- question/NN\n",
      "that/IN <--mark-- need/VBP\n",
      "we/PRP <--nsubj-- need/VBP\n",
      "need/VBP <--ccomp-- find/VBP\n",
      "the/DT <--det-- cooperation/NN\n",
      "cooperation/NN <--dobj-- need/VBP\n",
      "./. <--punct-- However/RB\n",
      "\n",
      "The/DT <--det-- goal/NN\n",
      "greater/JJR <--amod-- goal/NN\n",
      "our/PRP$ <--poss-- goal/NN\n",
      "goal/NN <--nsubj-- is/VBZ\n",
      "is/VBZ <--ROOT-- is/VBZ\n",
      ",/, <--punct-- is/VBZ\n",
      "the/DT <--det-- competition/NN\n",
      "more/JJR <--amod-- competition/NN\n",
      "competition/NN <--dobj-- need/VBP\n",
      "we/PRP <--nsubj-- need/VBP\n",
      "need/VBP <--ccomp-- is/VBZ\n",
      "./. <--punct-- is/VBZ\n",
      "\n",
      "Take/VB <--advcl-- is/VBZ\n",
      "Olympic/JJ <--amod-- games/NNS\n",
      "games/NNS <--dobj-- Take/VB\n",
      "which/WDT <--nsubj-- is/VBZ\n",
      "is/VBZ <--relcl-- games/NNS\n",
      "a/DT <--det-- form/NN\n",
      "form/NN <--attr-- is/VBZ\n",
      "of/IN <--prep-- form/NN\n",
      "competition/NN <--pobj-- of/IN\n",
      "for/IN <--prep-- competition/NN\n",
      "instance/NN <--pobj-- for/IN\n",
      ",/, <--punct-- is/VBZ\n",
      "it/PRP <--nsubj-- is/VBZ\n",
      "is/VBZ <--ROOT-- is/VBZ\n",
      "hard/JJ <--acomp-- is/VBZ\n",
      "to/TO <--aux-- imagine/VB\n",
      "imagine/VB <--xcomp-- is/VBZ\n",
      "how/WRB <--advmod-- win/VB\n",
      "an/DT <--det-- athlete/NN\n",
      "athlete/NN <--nsubj-- win/VB\n",
      "could/MD <--aux-- win/VB\n",
      "win/VB <--ccomp-- imagine/VB\n",
      "the/DT <--det-- game/NN\n",
      "game/NN <--dobj-- win/VB\n",
      "without/IN <--prep-- win/VB\n",
      "the/DT <--det-- training/NN\n",
      "training/NN <--pobj-- without/IN\n",
      "of/IN <--prep-- training/NN\n",
      "his/PRP$ <--poss-- coach/NN\n",
      "or/CC <--cc-- his/PRP$\n",
      "her/PRP$ <--poss-- coach/NN\n",
      "coach/NN <--pobj-- of/IN\n",
      ",/, <--punct-- is/VBZ\n",
      "and/CC <--cc-- is/VBZ\n",
      "the/DT <--det-- help/NN\n",
      "help/NN <--conj-- is/VBZ\n",
      "of/IN <--prep-- help/NN\n",
      "other/JJ <--amod-- staffs/NNS\n",
      "professional/JJ <--amod-- staffs/NNS\n",
      "staffs/NNS <--pobj-- of/IN\n",
      "such/JJ <--amod-- as/IN\n",
      "as/IN <--prep-- staffs/NNS\n",
      "the/DT <--det-- people/NNS\n",
      "people/NNS <--pobj-- as/IN\n",
      "who/WP <--nsubj-- take/VBP\n",
      "take/VBP <--relcl-- people/NNS\n",
      "care/NN <--dobj-- take/VBP\n",
      "of/IN <--prep-- take/VBP\n",
      "his/PRP$ <--poss-- diet/NN\n",
      "diet/NN <--pobj-- of/IN\n",
      ",/, <--punct-- help/NN\n",
      "and/CC <--cc-- help/NN\n",
      "those/DT <--conj-- help/NN\n",
      "who/WP <--nsubj-- are/VBP\n",
      "are/VBP <--relcl-- those/DT\n",
      "in/IN <--prep-- are/VBP\n",
      "charge/NN <--pobj-- in/IN\n",
      "of/IN <--prep-- charge/NN\n",
      "the/DT <--det-- care/NN\n",
      "medical/JJ <--amod-- care/NN\n",
      "care/NN <--pobj-- of/IN\n",
      "./. <--punct-- is/VBZ\n",
      "\n",
      "The/DT <--det-- winner/NN\n",
      "winner/NN <--nsubj-- is/VBZ\n",
      "is/VBZ <--ROOT-- is/VBZ\n",
      "the/DT <--det-- athlete/NN\n",
      "athlete/NN <--attr-- is/VBZ\n",
      "but/CC <--cc-- is/VBZ\n",
      "the/DT <--det-- success/NN\n",
      "success/NN <--nsubj-- belongs/VBZ\n",
      "belongs/VBZ <--conj-- is/VBZ\n",
      "to/IN <--prep-- belongs/VBZ\n",
      "the/DT <--det-- team/NN\n",
      "whole/JJ <--amod-- team/NN\n",
      "team/NN <--pobj-- to/IN\n",
      "./. <--punct-- belongs/VBZ\n",
      "\n",
      "Therefore/RB <--advmod-- be/VB\n",
      "without/IN <--prep-- be/VB\n",
      "the/DT <--det-- cooperation/NN\n",
      "cooperation/NN <--pobj-- without/IN\n",
      ",/, <--punct-- be/VB\n",
      "there/EX <--expl-- be/VB\n",
      "would/MD <--aux-- be/VB\n",
      "be/VB <--ROOT-- be/VB\n",
      "no/DT <--det-- victory/NN\n",
      "victory/NN <--attr-- be/VB\n",
      "of/IN <--prep-- victory/NN\n",
      "competition/NN <--pobj-- of/IN\n",
      "./. <--punct-- be/VB\n",
      "\n",
      "/_SP <---- ./.\n",
      "\n",
      "Consequently/RB <--advmod-- receive/VB\n",
      ",/, <--punct-- receive/VB\n",
      "no/RB <--neg-- matter/RB\n",
      "matter/RB <--advmod-- receive/VB\n",
      "from/IN <--prep-- matter/RB\n",
      "the/DT <--det-- view/NN\n",
      "view/NN <--pobj-- from/IN\n",
      "of/IN <--prep-- view/NN\n",
      "individual/JJ <--amod-- development/NN\n",
      "development/NN <--pobj-- of/IN\n",
      "or/CC <--cc-- development/NN\n",
      "the/DT <--det-- relationship/NN\n",
      "relationship/NN <--conj-- development/NN\n",
      "between/IN <--prep-- relationship/NN\n",
      "competition/NN <--pobj-- between/IN\n",
      "and/CC <--cc-- competition/NN\n",
      "cooperation/NN <--conj-- competition/NN\n",
      "we/PRP <--nsubj-- receive/VB\n",
      "can/MD <--aux-- receive/VB\n",
      "receive/VB <--ROOT-- receive/VB\n",
      "the/DT <--det-- conclusion/NN\n",
      "same/JJ <--amod-- conclusion/NN\n",
      "conclusion/NN <--dobj-- receive/VB\n",
      "that/IN <--mark-- is/VBZ\n",
      "a/DT <--det-- attitudes/NNS\n",
      "more/RBR <--advmod-- cooperative/JJ\n",
      "cooperative/JJ <--amod-- attitudes/NNS\n",
      "attitudes/NNS <--nsubj-- is/VBZ\n",
      "towards/IN <--prep-- attitudes/NNS\n",
      "life/NN <--pobj-- towards/IN\n",
      "is/VBZ <--acl-- conclusion/NN\n",
      "more/RBR <--advmod-- profitable/JJ\n",
      "profitable/JJ <--acomp-- is/VBZ\n",
      "in/IN <--prep-- is/VBZ\n",
      "one/PRP <--poss-- success/NN\n",
      "'s/POS <--case-- one/PRP\n",
      "success/NN <--pobj-- in/IN\n",
      "./. <--punct-- receive/VB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"This example shows how to navigate the parse tree including subtrees\n",
    "attached to a word.\n",
    "\n",
    "Based on issue #252:\n",
    "\"In the documents and tutorials the main thing I haven't found is\n",
    "examples on how to break sentences down into small sub thoughts/chunks. The\n",
    "noun_chunks is handy, but having examples on using the token.head to find small\n",
    "(near-complete) sentence chunks would be neat. Lets take the example sentence:\n",
    "\"displaCy uses CSS and JavaScript to show you how computers understand language\"\n",
    "\n",
    "This sentence has two main parts (XCOMP & CCOMP) according to the breakdown:\n",
    "[displaCy] uses CSS and Javascript [to + show]\n",
    "show you how computers understand [language]\n",
    "\n",
    "I'm assuming that we can use the token.head to build these groups.\"\n",
    "\n",
    "Compatible with: spaCy v2.0.0+\n",
    "Last tested with: v2.1.0\n",
    "\"\"\"\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import spacy\n",
    "\n",
    "\n",
    "# @plac.annotations(model=(\"Model to load\", \"positional\", None, str))\n",
    "def parse(model=\"en_core_web_sm\"):\n",
    "    nlp = spacy.load(model)\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "\n",
    "    doc = nlp(\n",
    "        \"displaCy uses CSS and JavaScript to show you how computers \"\n",
    "        \"understand language\"\n",
    "    )\n",
    "\n",
    "    # The easiest way is to find the head of the subtree you want, and then use\n",
    "    # the `.subtree`, `.children`, `.lefts` and `.rights` iterators. `.subtree`\n",
    "    # is the one that does what you're asking for most directly:\n",
    "    for word in doc:\n",
    "        if word.dep_ in (\"xcomp\", \"ccomp\"):\n",
    "            print(\"\".join(w.text_with_ws for w in word.subtree))\n",
    "\n",
    "    # It'd probably be better for `word.subtree` to return a `Span` object\n",
    "    # instead of a generator over the tokens. If you want the `Span` you can\n",
    "    # get it via the `.right_edge` and `.left_edge` properties. The `Span`\n",
    "    # object is nice because you can easily get a vector, merge it, etc.\n",
    "    for word in doc:\n",
    "        if word.dep_ in (\"xcomp\", \"ccomp\"):\n",
    "            subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
    "            print(subtree_span.text, \"|\", subtree_span.root.text)\n",
    "\n",
    "    # You might also want to select a head, and then select a start and end\n",
    "    # position by walking along its children. You could then take the\n",
    "    # `.left_edge` and `.right_edge` of those tokens, and use it to calculate\n",
    "    # a span.\n",
    "\n",
    "    \n",
    "\n",
    "    # Expected output:\n",
    "    # to show you how computers understand language\n",
    "    # how computers understand language\n",
    "    # to show you how computers understand language | show\n",
    "    # how computers understand language | understand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
