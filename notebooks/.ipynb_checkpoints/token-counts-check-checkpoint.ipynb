{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = pd.read_csv('../data/PE/train-test-split.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 402)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays_txt = []\n",
    "for file in sorted(glob.glob(\"../data/PE/*.txt\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_txt.append(essay)\n",
    "    \n",
    "essays_ann = []\n",
    "for file in sorted(glob.glob(\"../data/PE/*.ann\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_ann.append(essay)\n",
    "    \n",
    "len(glob.glob(\"../data/PE/*.txt\")), len(glob.glob(\"../data/PE/*.ann\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Arg Segments Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_segments = []\n",
    "\n",
    "for essay in essays_ann:    \n",
    "    segments = []\n",
    "    \n",
    "    for line in essay:\n",
    "        if line[0] == 'T':\n",
    "            _, label_s_e, text = line.rstrip().split('\\t')\n",
    "            label, start, end = label_s_e.split()\n",
    "            segments.append((label, int(start), int(end), text))\n",
    "            \n",
    "    segments.sort(key = lambda element : element[1])\n",
    "    essays_segments.append(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting BIO segments for each essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_BIO = []\n",
    "for text, segments in zip(essays_txt, essays_segments):\n",
    "    B_I, O = [], []\n",
    "    text_str = ''.join(text)\n",
    "    \n",
    "    #first non Arg segment\n",
    "    assert segments[0][1] != 0\n",
    "    O.append(text_str[:segments[0][1]])\n",
    "    \n",
    "    # looping through arg segments of an essay\n",
    "    for i, seg in enumerate(segments):\n",
    "        _, start, end, seg_text = seg\n",
    "        assert text_str[start:end] == seg_text\n",
    "        \n",
    "        B_I.append(text_str[start:end])\n",
    "        \n",
    "        # O text segment starts from end of this Arg segment till start of next Arg segment\n",
    "        '''TODO: I need to check for cases where there are two adjacent Arg segments'''\n",
    "        if i+1 < len(segments):\n",
    "            O.append(text_str[end: segments[i+1][1]])\n",
    "    \n",
    "    #last non Arg segment if exists\n",
    "    if segments[-1][2] < len(text_str):\n",
    "        O.append(text_str[segments[-1][2]:])\n",
    "    \n",
    "    essays_BIO.append([B_I, O])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count BIO tokens in each essay using multiple tokenization teqchniques  (srt.split, nltk, stanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIO_counts = []\n",
    "for BI, O in essays_BIO:\n",
    "    B_count = len(BI)\n",
    "    \n",
    "    I_count = 0\n",
    "    for text in BI:\n",
    "#         I_count += len(text.split()) - 1\n",
    "        I_count += len(word_tokenize(text)) - 1\n",
    "    \n",
    "    O_count = 0\n",
    "    for text in O:\n",
    "#         O_count += len(text.split())\n",
    "        O_count += len(word_tokenize(text))\n",
    "    \n",
    "    BIO_counts.append((B_count, I_count, O_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special handling of counting '\\n' needed?\n",
    "new_line_counts = []\n",
    "for BI, O in essays_BIO:\n",
    "    new_line = 0\n",
    "    for text in O:\n",
    "        new_line += len([c for c in text if c == '\\n'])\n",
    "    new_line_counts.append(new_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford's CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /home/research/interns/talhindi/tools/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-2e1aa8ee117243b4.props -preload tokenize\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = '/home/research/interns/talhindi/tools/stanford-corenlp-4.0.0'\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "client = CoreNLPClient(annotators=['tokenize'], timeout=30000, memory='16G')\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving stanford's counts because they are heavy to compute\n",
    "BIO_counts_stanford = []\n",
    "for BI, O in essays_BIO:\n",
    "    B_count = len(BI)\n",
    "    \n",
    "    I_count = 0\n",
    "    for text in BI:\n",
    "        ann = client.annotate(text)\n",
    "        sent_tokens = 0\n",
    "        for sent in ann.sentence:\n",
    "            sent_tokens += len(sent.token)\n",
    "        I_count += sent_tokens + len(ann.sentencelessToken) \n",
    "    \n",
    "    O_count = 0\n",
    "    for text in O:\n",
    "        ann = client.annotate(text)\n",
    "        sent_tokens = 0\n",
    "        for sent in ann.sentence:\n",
    "            sent_tokens += len(sent.token)\n",
    "        O_count += sent_tokens + len(ann.sentencelessToken) \n",
    "        \n",
    "    \n",
    "    BIO_counts_stanford.append((B_count, I_count, O_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Total number of Segments from each type in Training and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\t  I\t  O\n",
      "train: 4823\t74716\t37937\n",
      "test:  1266\t18565\t9375\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "train_B, train_I, train_O = 0, 0, 0\n",
    "test_B, test_I, test_O = 0, 0, 0\n",
    "\n",
    "for group, counts in zip(train_test_split.SET, BIO_counts):\n",
    "    if group == \"TRAIN\":\n",
    "        train_B += counts[0]\n",
    "        train_I += counts[1]\n",
    "        train_O += counts[2]\n",
    "    else:\n",
    "        test_B += counts[0]\n",
    "        test_I += counts[1]\n",
    "        test_O += counts[2]\n",
    "\n",
    "print('\\tB\\t  I\\t  O')        \n",
    "print('train: {}\\t{}\\t{}'.format(train_B, train_I, train_O))\n",
    "print('test:  {}\\t{}\\t{}'.format(test_B, test_I, test_O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\t  I\t  O\n",
      "train: 4823\t74716\t39810\n",
      "test:  1266\t18565\t9841\n"
     ]
    }
   ],
   "source": [
    "# nltk without \n",
    "train_B, train_I, train_O = 0, 0, 0\n",
    "test_B, test_I, test_O = 0, 0, 0\n",
    "\n",
    "for group, counts, nl_counts in zip(train_test_split.SET, BIO_counts, new_line_counts):\n",
    "    if group == \"TRAIN\":\n",
    "        train_B += counts[0]\n",
    "        train_I += counts[1]\n",
    "        train_O += counts[2] + nl_counts\n",
    "    else:\n",
    "        test_B += counts[0]\n",
    "        test_I += counts[1]\n",
    "        test_O += counts[2]  + nl_counts\n",
    "\n",
    "print('\\tB\\t  I\\t  O')        \n",
    "print('train: {}\\t{}\\t{}'.format(train_B, train_I, train_O))\n",
    "print('test:  {}\\t{}\\t{}'.format(test_B, test_I, test_O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\t  I\t  O\n",
      "train: 4823\t75220\t34765\n",
      "test:  1266\t18680\t8519\n"
     ]
    }
   ],
   "source": [
    "# corenlp, only sentencelesstokens\n",
    "train_B, train_I, train_O = 0, 0, 0\n",
    "test_B, test_I, test_O = 0, 0, 0\n",
    "\n",
    "for group, counts, nl_counts in zip(train_test_split.SET, BIO_counts_stanford, new_line_counts):\n",
    "    if group == \"TRAIN\":\n",
    "        train_B += counts[0]\n",
    "        train_I += counts[1]\n",
    "        train_O += counts[2] + nl_counts\n",
    "    else:\n",
    "        test_B += counts[0]\n",
    "        test_I += counts[1]\n",
    "        test_O += counts[2]  + nl_counts\n",
    "\n",
    "print('\\tB\\t  I\\t  O')        \n",
    "print('train: {}\\t{}\\t{}'.format(train_B, train_I, train_O))\n",
    "print('test:  {}\\t{}\\t{}'.format(test_B, test_I, test_O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\t  I\t  O\n",
      "train: 4823\t80043\t38037\n",
      "test:  1266\t19946\t9399\n"
     ]
    }
   ],
   "source": [
    "# core\n",
    "train_B, train_I, train_O = 0, 0, 0\n",
    "test_B, test_I, test_O = 0, 0, 0\n",
    "\n",
    "for group, counts in zip(train_test_split.SET, BIO_counts_stanford):\n",
    "    if group == \"TRAIN\":\n",
    "        train_B += counts[0]\n",
    "        train_I += counts[1]\n",
    "        train_O += counts[2]\n",
    "    else:\n",
    "        test_B += counts[0]\n",
    "        test_I += counts[1]\n",
    "        test_O += counts[2]\n",
    "\n",
    "print('\\tB\\t  I\\t  O')        \n",
    "print('train: {}\\t{}\\t{}'.format(train_B, train_I, train_O))\n",
    "print('test:  {}\\t{}\\t{}'.format(test_B, test_I, test_O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tB\t  I\t  O\n",
      "train: 4823\t80043\t39910\n",
      "test:  1266\t19946\t9865\n"
     ]
    }
   ],
   "source": [
    "train_B, train_I, train_O = 0, 0, 0\n",
    "test_B, test_I, test_O = 0, 0, 0\n",
    "\n",
    "for group, counts, nl_counts in zip(train_test_split.SET, BIO_counts_stanford, new_line_counts):\n",
    "    if group == \"TRAIN\":\n",
    "        train_B += counts[0]\n",
    "        train_I += counts[1]\n",
    "        train_O += counts[2] + nl_counts\n",
    "    else:\n",
    "        test_B += counts[0]\n",
    "        test_I += counts[1]\n",
    "        test_O += counts[2]  + nl_counts\n",
    "\n",
    "print('\\tB\\t  I\\t  O')        \n",
    "print('train: {}\\t{}\\t{}'.format(train_B, train_I, train_O))\n",
    "print('test:  {}\\t{}\\t{}'.format(test_B, test_I, test_O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
