{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import copy\n",
    "\n",
    "import os\n",
    "os.environ[\"CORENLP_HOME\"] = '/Users/talhindi/Downloads/stanford-corenlp-4.0.0'\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import stanza\n",
    "from stanza.server import CoreNLPClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CoreNLP_pb2\n",
    "import corenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = pd.read_csv('../data/SG2017/train-test-split.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_txt_prg_list = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017/*.txt\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_txt_prg_list.append(essay)\n",
    "\n",
    "essay_txt_str = []\n",
    "for essay in essays_txt_prg_list:\n",
    "    essay_txt_str.append(''.join(essay))\n",
    "    \n",
    "essays_ann = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017/*.ann\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_ann.append(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_segments = []\n",
    "\n",
    "for essay in essays_ann:    \n",
    "    segments = []\n",
    "    \n",
    "    for line in essay:\n",
    "        if line[0] == 'T':\n",
    "            _, label_s_e, text = line.rstrip().split('\\t')\n",
    "            label, start, end = label_s_e.split()\n",
    "            segments.append((label, int(start), int(end), text))\n",
    "            \n",
    "    segments.sort(key = lambda element : element[1])\n",
    "    essays_segments.append(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CoreNLPClient(annotators=['tokenize', 'ssplit'], timeout=30000, memory='16G')\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_txt_str_tok, essays_txt_prg_list_tok = [], []\n",
    "\n",
    "for essay_doc, essay_prgs in zip (essay_txt_str, essays_txt_prg_list):\n",
    "    essay_txt_str_tok.append(client.annotate(essay_doc))\n",
    "\n",
    "    prg_list_tok = []\n",
    "    for prg in essay_prgs:\n",
    "        prg_list_tok.append(client.annotate(prg))\n",
    "    \n",
    "    essays_txt_prg_list_tok.append(prg_list_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't pickle!\n",
    "pickle.dump(essay_txt_str_tok, open('../pkl/SG2017/essay_txt_str_tok.p', 'wb'))\n",
    "pickle.dump(essays_txt_prg_list_tok, open('../pkl/SG2017/essays_txt_prg_list_tok.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(essay_doc_tok, segments):\n",
    "    '''O = 0, Arg-B = 1, Arg-I = 2'''\n",
    "    \n",
    "    doc_len = sum([len(sent.token) for sent in essay_doc_tok.sentence])\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "    arg_seg_starts = [start for arg_type, start, end, text in segments]\n",
    "    \n",
    "    for sent in essay_doc_tok.sentence:\n",
    "        for token in sent.token:\n",
    "            arg_I_token = False\n",
    "\n",
    "            if token.beginChar in arg_seg_starts:\n",
    "#                 labels.append('B')\n",
    "                labels.append(1.0)\n",
    "                tokens.append(token.word)\n",
    "                assert token.word in segments[arg_seg_starts.index(token.beginChar)][-1]\n",
    "            else:\n",
    "                for _, start, end, _ in segments:\n",
    "                    if token.beginChar > start and token.endChar <= end:\n",
    "#                         labels.append('I')\n",
    "                        labels.append(2.0)\n",
    "                        tokens.append(token.word)\n",
    "                        arg_I_token = True\n",
    "                if not arg_I_token:\n",
    "#                     labels.append('O')\n",
    "                    labels.append(0.0)\n",
    "                    tokens.append(token.word)\n",
    "\n",
    "    assert len(labels) == doc_len\n",
    "    return tokens, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting labels from each type\n",
    "token_labels = []\n",
    "train_BIO = defaultdict(int)\n",
    "test_BIO = defaultdict(int)\n",
    "\n",
    "for doc_tok, segments, group in zip(essay_txt_str_tok, essays_segments, train_test_split.SET):\n",
    "    tokens, labels = get_labels(doc_tok, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        for label in  labels:\n",
    "            train_BIO[label] += 1\n",
    "    else:\n",
    "        for label in  labels:\n",
    "            test_BIO[label] += 1\n",
    "    \n",
    "train_BIO,test_BIO\n",
    "# defaultdict(int, {0.0: 38039, 1.0: 4822, 2.0: 75216}),\n",
    "# defaultdict(int, {0.0: 9400, 1.0: 1266, 2.0: 18678})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''token position: \n",
    "        Token present in introduction or conclusion;  ----> ignore for now\n",
    "        token is first or last token in sentence; \n",
    "        relative and absolute token position in document, paragraph and sentence'''\n",
    "\n",
    "def get_positions(essay_tok):\n",
    "    \n",
    "    doc_len, prg_lengths, sent_lengths = get_lengths(essay_tok)\n",
    "    \n",
    "    positions = []\n",
    "    doc_pos, sent_id = 0, 0\n",
    "    \n",
    "    for prg_id, prg in enumerate(essay_tok):\n",
    "        \n",
    "        prg_pos = 0\n",
    "        for sent in prg.sentence:\n",
    "            \n",
    "            sent_pos = 0\n",
    "            for i, token in enumerate(sent.token):\n",
    "                \n",
    "                if i == 0: \n",
    "                    positions.append({'doc_abs_pos': doc_pos, 'prg_abs_pos': prg_pos, 'sent_abs_pos': sent_pos,\n",
    "                                'doc_rel_pos': round(doc_pos/doc_len,4), \n",
    "                                'prg_rel_pos': round(prg_pos/prg_lengths[prg_id],4),\n",
    "                                'sent_rel_pos': round(sent_pos/sent_lengths[sent_id],4), 'is_first_in_sent': 1.0})\n",
    "                elif i == len(sent.token)-1: \n",
    "                    positions.append({'doc_abs_pos': doc_pos, 'prg_abs_pos': prg_pos, 'sent_abs_pos': sent_pos,\n",
    "                                'doc_rel_pos': round(doc_pos/doc_len,4), \n",
    "                                'prg_rel_pos': round(prg_pos/prg_lengths[prg_id],4),\n",
    "                                'sent_rel_pos': round(sent_pos/sent_lengths[sent_id],4), 'is_last_in_sent': 1.0})\n",
    "                else:\n",
    "                    positions.append({'doc_abs_pos': doc_pos, 'prg_abs_pos': prg_pos, 'sent_abs_pos': sent_pos,\n",
    "                                'doc_rel_pos': round(doc_pos/doc_len,4), \n",
    "                                'prg_rel_pos': round(prg_pos/prg_lengths[prg_id],4),\n",
    "                                'sent_rel_pos': round(sent_pos/sent_lengths[sent_id],4)})\n",
    "                \n",
    "                doc_pos += 1; prg_pos += 1; sent_pos += 1;\n",
    "            \n",
    "            sent_id += 1\n",
    "    \n",
    "    return positions\n",
    "\n",
    "\n",
    "\n",
    "def get_lengths(essay_tok):\n",
    "    '''Returns essay length, and length of each paragraph and sentence in the essay'''\n",
    "    doc_len, prg_lengths, sent_lengths = 0, [], []\n",
    "\n",
    "    for prg in essay_tok:\n",
    "\n",
    "        prg_len = 0\n",
    "        for sent in prg.sentence:\n",
    "\n",
    "            sent_len = 0\n",
    "            for token in sent.token:\n",
    "                doc_len += 1; prg_len += 1; sent_len += 1\n",
    "            sent_lengths.append(sent_len)\n",
    "\n",
    "        prg_lengths.append(prg_len)\n",
    "\n",
    "    assert doc_len == sum(prg_lengths) == sum(sent_lengths)\n",
    "    \n",
    "    return doc_len, prg_lengths, sent_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''punctuation:\n",
    "        Token precedes or follows any punctuation, full stop, comma and semicolon;\n",
    "        token is any punctuation or full stop'''      \n",
    "\n",
    "def get_punc_features(essay_doc_tok):\n",
    "    \n",
    "    token_features = {}\n",
    "    set_reset_features(token_features)\n",
    "    \n",
    "    tokens, features = [], []\n",
    "    for sent_id, sent in enumerate(essay_doc_tok.sentence):\n",
    "        for token_id, token in enumerate(sent.token):\n",
    "            tokens.append(token.word)\n",
    "    \n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in string.punctuation:\n",
    "                token_features['punc'] = True\n",
    "                if token == \".\": token_features['fullstop'] = True\n",
    "                    \n",
    "        if i == 0:\n",
    "            next_punc_features(tokens[i+1], token_features)\n",
    "        elif i == len(tokens)-1:\n",
    "            prev_punc_features(tokens[i-1], token_features)\n",
    "        else:\n",
    "            prev_punc_features(tokens[i-1], token_features)\n",
    "            next_punc_features(tokens[i+1], token_features)\n",
    "\n",
    "        # adding features of this token to the list of features\n",
    "        features.append(copy.deepcopy(token_features))\n",
    "\n",
    "        # resetting features to process the next token\n",
    "        set_reset_features(token_features)\n",
    "        \n",
    "    return features\n",
    "\n",
    "    \n",
    "def set_reset_features(token_features):\n",
    "    token_features['punc'], token_features['fullstop'] = False, False\n",
    "    token_features['punc_prev'], token_features['fullstop_prev'] = False, False\n",
    "    token_features['comma_prev'], token_features['semicolon_prev'] = False, False\n",
    "    token_features['punc_next'], token_features['fullstop_next'] = False, False\n",
    "    token_features['comma_next'], token_features['semicolon_next'] = False, False\n",
    "\n",
    "def prev_punc_features(prev_token, token_features):    \n",
    "    if prev_token in string.punctuation:\n",
    "        token_features['punc_prev'] = True\n",
    "        if prev_token == \".\":\n",
    "            token_features['fullstop_prev'] = True\n",
    "        if prev_token == \",\":\n",
    "            token_features['comma_prev'] = True\n",
    "        if prev_token == \";\":\n",
    "            token_features['semicolon_prev'] = True\n",
    "\n",
    "def next_punc_features(next_token, token_features):\n",
    "    if next_token in string.punctuation:\n",
    "        token_features['punc_next'] = True\n",
    "        if next_token == \".\":\n",
    "            token_features['fullstop_next'] = True\n",
    "        if next_token == \",\":\n",
    "            token_features['comma_next'] = True\n",
    "        if next_token == \";\":\n",
    "            token_features['semicolon_next'] = True\n",
    "\n",
    "def punc_features_to_json(this_token_features):\n",
    "    json_features = {}\n",
    "    \n",
    "    for key in sorted(this_token_features.keys()):\n",
    "        value = this_token_features[key]\n",
    "        if value:\n",
    "            json_features[key] = 1.0\n",
    "        \n",
    "    return json.dumps(json_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''position of covering sentence\n",
    "        Absolute and relative position of the token’s covering sentence in the document and paragraph'''\n",
    "\n",
    "def tok_sent_pos(essay_tok):\n",
    "    \n",
    "    prg_lengths = [len(prg.sentence) for prg in essay_tok]\n",
    "    doc_len = sum(prg_lengths)\n",
    "    \n",
    "    doc_pos, positions = 0, []\n",
    "    for prg_id, prg in enumerate(essay_tok):\n",
    "        \n",
    "        prg_pos = 0\n",
    "        for sent in prg.sentence:\n",
    "            for i, token in enumerate(sent.token):\n",
    "                positions.append({'sent_doc_abs_pos': doc_pos, 'sent_prg_abs_pos': prg_pos,\n",
    "                                'sent_doc_rel_pos': round(doc_pos/doc_len,4), \n",
    "                                'sent_prg_rel_pos': round(prg_pos/prg_lengths[prg_id],4)})\n",
    "            \n",
    "            doc_pos += 1; prg_pos += 1;\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position features\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_position.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_position.jsonlines', 'w')\n",
    "\n",
    "for doc_tok, prg_list_tok, segments, group in zip(essay_txt_str_tok, essays_txt_prg_list_tok, essays_segments, train_test_split.SET):\n",
    "    \n",
    "    features = get_positions(prg_list_tok)\n",
    "    tokens, labels = get_labels(doc_tok, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_position.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_position.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punc features\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_punc.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_punc.jsonlines', 'w')\n",
    "\n",
    "for doc_tok, segments, group in zip(essay_txt_str_tok, essays_segments, train_test_split.SET):\n",
    "    \n",
    "    features = get_punc_features(doc_tok)\n",
    "    tokens, labels = get_labels(doc_tok, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_punc.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, punc_features_to_json(f), token_id))\n",
    "                token_id +=1\n",
    "                \n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_punc.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, punc_features_to_json(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position of covering sentence\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_position_sent.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_position_sent.jsonlines', 'w')\n",
    "\n",
    "for doc_tok, prg_list_tok, segments, group in zip(essay_txt_str_tok, essays_txt_prg_list_tok, essays_segments, train_test_split.SET):\n",
    "    \n",
    "    features = tok_sent_pos(prg_list_tok)\n",
    "    tokens, labels = get_labels(doc_tok, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_position_sent.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_position_sent.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/talhindi/Downloads/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-efef7f818a764f3d.props -preload pos,parse\n"
     ]
    }
   ],
   "source": [
    "client = CoreNLPClient(annotators=['pos', 'parse'], timeout=30000, memory='16G')\n",
    "client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_txt_str_pos_parse, essays_txt_prg_list_pos_parse = [], []\n",
    "\n",
    "for essay_doc, essay_prgs in zip (essay_txt_str, essays_txt_prg_list):\n",
    "    essay_txt_str_pos_parse.append(client.annotate(essay_doc))\n",
    "\n",
    "    prg_list_pos_parse = []\n",
    "    for prg in essay_prgs:\n",
    "        prg_list_pos_parse.append(client.annotate(prg))\n",
    "    \n",
    "    essays_txt_prg_list_pos_parse.append(prg_list_pos_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = client.annotate(essay_txt_str[0])\n",
    "\n",
    "prg_list_pos_parse = []\n",
    "for prg in essays_txt_prg_list[0]:\n",
    "    prg_list_pos_parse.append(client.annotate(prg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_parse = doc.sentence[0].parseTree\n",
    "child = constituency_parse.child[0].child[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "********\n",
      "4\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "1\n",
      "--\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "0\n",
      "---\n",
      "****\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "1\n",
      "--\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "1\n",
      "---\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "****\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "2\n",
      "--\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "1\n",
      "---\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "2\n",
      "---\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "****\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "1\n",
      "--\n",
      "<class 'CoreNLP_pb2.ParseTree'>\n",
      "0\n",
      "---\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "print(type(constituency_parse.child[0]))\n",
    "print('********')\n",
    "print(len(constituency_parse.child[0].child))\n",
    "for child in constituency_parse.child[0].child:\n",
    "    print(type(child))\n",
    "    print(len(child.child))\n",
    "    print('--')\n",
    "    for grandchild in child.child:\n",
    "        print(type(grandchild))\n",
    "        print(len(grandchild.child))\n",
    "        print('---')\n",
    "        for grandGrandChild in grandchild.child:\n",
    "            print(type(grandGrandChild))\n",
    "        \n",
    "    print('****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_Tree(sentence):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CoreNLPClient(annotators=['tokenize', 'mwt', 'pos', 'lemma', 'depparse'], timeout=30000, memory='16G', port=9001)\n",
    "client.start()\n",
    "\n",
    "doc = client.annotate(essay_txt_str[0])\n",
    "sentence = doc.sentence[0]\n",
    "client.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should students be taught to compete or to cooperate ? \n",
      "1 2 3 4 5 6 7 8 9 10 4 1\n",
      "4 2\n",
      "4 3\n",
      "4 6\n",
      "4 10\n",
      "6 5\n",
      "6 9\n",
      "9 7\n",
      "9 8\n"
     ]
    }
   ],
   "source": [
    "for token in sentence.token:\n",
    "    print(token.word, end=' ')\n",
    "print()    \n",
    "for node in sentence.basicDependencies.node:\n",
    "    print(node.index,  end=' ')\n",
    "\n",
    "for edge in sentence.basicDependencies.edge:\n",
    "    print(edge.source, edge.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(t):\n",
    "    try:\n",
    "#         t.value()\n",
    "        print('(', t.value(), end=\" \")\n",
    "    except AttributeError:\n",
    "        print()\n",
    "#         print(t, end=\" \")\n",
    "#     else:\n",
    "        # Now we know that t.node is defined\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "        print(')', end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(essay_doc_pos):\n",
    "    '''Part-of-speech: The token’s part-of-speech'''\n",
    "    pos_features = []\n",
    "    for sent in essay_doc_pos.sentence:\n",
    "        for token in sent.token:\n",
    "            pos_features.append({'pos_{}'.format(token.pos): 1.0})\n",
    "    \n",
    "    return pos_features\n",
    "            \n",
    "\n",
    "def get_LCA():\n",
    "    '''Lowest common ancestor (LCA):\n",
    "        Normalized length of the path to the LCA with the *following* and *preceding* token in the parse tree'''\n",
    "    pass\n",
    "\n",
    "def get_LCA_type():\n",
    "    '''LCA types: The two constituent types of the LCA of the current token and its preceding and following token'''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tags\n",
    "\n",
    "token_id = 0\n",
    "open('../features/SG2017_train/token_pos.jsonlines', 'w')\n",
    "open('../features/SG2017_test/token_pos.jsonlines', 'w')\n",
    "\n",
    "for doc_tok, segments, group in zip(essay_txt_str_tok, essays_segments, train_test_split.SET):\n",
    "    \n",
    "    features = get_pos(doc_tok)\n",
    "    tokens, labels = get_labels(doc_tok, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../features/SG2017_train/token_pos.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1\n",
    "    else:\n",
    "        with open('../features/SG2017_test/token_pos.jsonlines', 'a') as file:\n",
    "            for f, l in zip(features, labels):\n",
    "                file.write('{{\"y\": {}, \"x\": {}, \"id\": {}}}\\n'.format(l, json.dumps(f), token_id))\n",
    "                token_id +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LexSyn and Probability Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LexSyn 1:\n",
    "        We use lexical head projection rules (Collins 2003) implemented in the Stanford tool suite\n",
    "        to lexicalize the constituent parse tree. \n",
    "        For each token t, we extract its uppermost node n in the parse tree \n",
    "        with the lexical head t and define a lexico- syntactic feature as \n",
    "        the combination of t and the constituent type of n.'''\n",
    "\n",
    "'''LexSyn 2:\n",
    "        We also consider the child node of n in the path to t and its right sibling, \n",
    "        and combine their lexical heads and constituent types as described by Soricut and Marcu (2003).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''probability-feature:\n",
    "        is the conditional probability of the current token t_i \n",
    "        being the beginning of an argument component (“Arg-B”) given its preceding tokens.\n",
    "        using MLE on the training data\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dump of old get_punc function'''\n",
    "\n",
    "    #checking if the token is a word or punctuations\n",
    "    if token.word in string.punctuation:\n",
    "        token_features['punc'] = True\n",
    "        if token.word == \".\": token_features['fullstop'] = True\n",
    "\n",
    "    # no prev to the first token in the essay\n",
    "    if sent_id == 0 and token_id == 0:\n",
    "        prev_token = ''\n",
    "        next_token = sent.token[1].word\n",
    "        next_punc_features(next_token, token_features)\n",
    "\n",
    "    # no next to the last token in the essay\n",
    "    elif sent_id == len(essay_doc_tok.sentence)-1 and token_id == len(sent.token)-1:\n",
    "        prev_token = sent.token[-2].word\n",
    "        prev_punc_features(prev_token, token_features)\n",
    "        next_token = ''\n",
    "\n",
    "    # token is neither first nor last in the essay\n",
    "    else:\n",
    "\n",
    "        # token is neither first not last in the sentence\n",
    "        if token_id > 0 and token_id < len(sent.token)-1:\n",
    "            prev_token = sent.token[token_id-1].word\n",
    "            next_token = sent.token[token_id+1].word\n",
    "\n",
    "        # token is first in the sentence\n",
    "        elif token_id == 0:\n",
    "            prev_token = essay_doc_tok.sentence[sent_id-1].token[-1].word\n",
    "            next_token = sent.token[token_id+1].word\n",
    "\n",
    "        # token is last in the sentence\n",
    "        elif token_id == len(sent.token)-1:\n",
    "            prev_token = sent.token[token_id-1].word\n",
    "            next_token = essay_doc_tok.sentence[sent_id+1].token[0].word\n",
    "\n",
    "        # this should never get executed \n",
    "        else:\n",
    "            print('something is wrong with token_id {} in sent_id {}'.format(token_id, sent_id))\n",
    "\n",
    "        prev_punc_features(prev_token, token_features)\n",
    "        next_punc_features(next_token, token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx16G -cp /Users/talhindi/Downloads/stanford-corenlp-4.0.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-0e1b40fd00ca471e.props -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref\n",
      "---\n",
      "constituency parse of first sentence\n",
      "child {\n",
      "  child {\n",
      "    child {\n",
      "      value: \"Should\"\n",
      "    }\n",
      "    value: \"MD\"\n",
      "  }\n",
      "  child {\n",
      "    child {\n",
      "      child {\n",
      "        value: \"students\"\n",
      "      }\n",
      "      value: \"NNS\"\n",
      "    }\n",
      "    value: \"NP\"\n",
      "  }\n",
      "  child {\n",
      "    child {\n",
      "      child {\n",
      "        value: \"be\"\n",
      "      }\n",
      "      value: \"VB\"\n",
      "    }\n",
      "    child {\n",
      "      child {\n",
      "        child {\n",
      "          value: \"taught\"\n",
      "        }\n",
      "        value: \"VBN\"\n",
      "      }\n",
      "      child {\n",
      "        child {\n",
      "          child {\n",
      "            child {\n",
      "              child {\n",
      "                value: \"to\"\n",
      "              }\n",
      "              value: \"TO\"\n",
      "            }\n",
      "            child {\n",
      "              child {\n",
      "                child {\n",
      "                  value: \"compete\"\n",
      "                }\n",
      "                value: \"VB\"\n",
      "              }\n",
      "              value: \"VP\"\n",
      "            }\n",
      "            value: \"VP\"\n",
      "          }\n",
      "          child {\n",
      "            child {\n",
      "              value: \"or\"\n",
      "            }\n",
      "            value: \"CC\"\n",
      "          }\n",
      "          child {\n",
      "            child {\n",
      "              child {\n",
      "                value: \"to\"\n",
      "              }\n",
      "              value: \"TO\"\n",
      "            }\n",
      "            child {\n",
      "              child {\n",
      "                child {\n",
      "                  value: \"cooperate\"\n",
      "                }\n",
      "                value: \"VB\"\n",
      "              }\n",
      "              value: \"VP\"\n",
      "            }\n",
      "            value: \"VP\"\n",
      "          }\n",
      "          value: \"VP\"\n",
      "        }\n",
      "        value: \"S\"\n",
      "      }\n",
      "      value: \"VP\"\n",
      "    }\n",
      "    value: \"VP\"\n",
      "  }\n",
      "  child {\n",
      "    child {\n",
      "      value: \"?\"\n",
      "    }\n",
      "    value: \".\"\n",
      "  }\n",
      "  value: \"SQ\"\n",
      "}\n",
      "value: \"ROOT\"\n",
      "score: 838.8004455566406\n",
      "\n",
      "---\n",
      "first subtree of constituency parse\n",
      "child {\n",
      "  child {\n",
      "    value: \"Should\"\n",
      "  }\n",
      "  value: \"MD\"\n",
      "}\n",
      "child {\n",
      "  child {\n",
      "    child {\n",
      "      value: \"students\"\n",
      "    }\n",
      "    value: \"NNS\"\n",
      "  }\n",
      "  value: \"NP\"\n",
      "}\n",
      "child {\n",
      "  child {\n",
      "    child {\n",
      "      value: \"be\"\n",
      "    }\n",
      "    value: \"VB\"\n",
      "  }\n",
      "  child {\n",
      "    child {\n",
      "      child {\n",
      "        value: \"taught\"\n",
      "      }\n",
      "      value: \"VBN\"\n",
      "    }\n",
      "    child {\n",
      "      child {\n",
      "        child {\n",
      "          child {\n",
      "            child {\n",
      "              value: \"to\"\n",
      "            }\n",
      "            value: \"TO\"\n",
      "          }\n",
      "          child {\n",
      "            child {\n",
      "              child {\n",
      "                value: \"compete\"\n",
      "              }\n",
      "              value: \"VB\"\n",
      "            }\n",
      "            value: \"VP\"\n",
      "          }\n",
      "          value: \"VP\"\n",
      "        }\n",
      "        child {\n",
      "          child {\n",
      "            value: \"or\"\n",
      "          }\n",
      "          value: \"CC\"\n",
      "        }\n",
      "        child {\n",
      "          child {\n",
      "            child {\n",
      "              value: \"to\"\n",
      "            }\n",
      "            value: \"TO\"\n",
      "          }\n",
      "          child {\n",
      "            child {\n",
      "              child {\n",
      "                value: \"cooperate\"\n",
      "              }\n",
      "              value: \"VB\"\n",
      "            }\n",
      "            value: \"VP\"\n",
      "          }\n",
      "          value: \"VP\"\n",
      "        }\n",
      "        value: \"VP\"\n",
      "      }\n",
      "      value: \"S\"\n",
      "    }\n",
      "    value: \"VP\"\n",
      "  }\n",
      "  value: \"VP\"\n",
      "}\n",
      "child {\n",
      "  child {\n",
      "    value: \"?\"\n",
      "  }\n",
      "  value: \".\"\n",
      "}\n",
      "value: \"SQ\"\n",
      "\n",
      "---\n",
      "value of first subtree of constituency parse\n",
      "SQ\n",
      "---\n",
      "dependency parse of first sentence\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 1\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 2\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 3\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 4\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 5\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 6\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 7\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 8\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 9\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 10\n",
      "}\n",
      "edge {\n",
      "  source: 4\n",
      "  target: 1\n",
      "  dep: \"aux\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 4\n",
      "  target: 2\n",
      "  dep: \"nsubj:pass\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 4\n",
      "  target: 3\n",
      "  dep: \"aux:pass\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 4\n",
      "  target: 6\n",
      "  dep: \"xcomp\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 4\n",
      "  target: 10\n",
      "  dep: \"punct\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 6\n",
      "  target: 5\n",
      "  dep: \"mark\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 6\n",
      "  target: 9\n",
      "  dep: \"conj\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 9\n",
      "  target: 7\n",
      "  dep: \"cc\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "edge {\n",
      "  source: 9\n",
      "  target: 8\n",
      "  dep: \"mark\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: UniversalEnglish\n",
      "}\n",
      "root: 4\n",
      "\n",
      "---\n",
      "first token of first sentence\n",
      "word: \"Should\"\n",
      "pos: \"MD\"\n",
      "value: \"Should\"\n",
      "before: \"\"\n",
      "after: \" \"\n",
      "originalText: \"Should\"\n",
      "ner: \"O\"\n",
      "lemma: \"should\"\n",
      "beginChar: 0\n",
      "endChar: 6\n",
      "utterance: 0\n",
      "speaker: \"PER0\"\n",
      "tokenBeginIndex: 0\n",
      "tokenEndIndex: 1\n",
      "hasXmlContext: false\n",
      "isNewline: false\n",
      "coarseNER: \"O\"\n",
      "fineGrainedNER: \"O\"\n",
      "nerLabelProbs: \"O=0.9997051562423394\"\n",
      "\n",
      "---\n",
      "part of speech tag of token\n",
      "MD\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index (0) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-b4b1d8b5dd87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# get the first sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# get the constituency parse of the first sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index (0) out of range"
     ]
    }
   ],
   "source": [
    "with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], timeout=30000, memory='16G') as client:\n",
    "    for prg in essays_txt_prg_list[0]:\n",
    "        # submit the request to the server\n",
    "        ann = client.annotate(prg.rstrip())\n",
    "\n",
    "        # get the first sentence\n",
    "        sentence = ann.sentence[0]\n",
    "\n",
    "        # get the constituency parse of the first sentence\n",
    "        print('---')\n",
    "        print('constituency parse of first sentence')\n",
    "        constituency_parse = sentence.parseTree\n",
    "        print(constituency_parse)\n",
    "\n",
    "        # get the first subtree of the constituency parse\n",
    "        print('---')\n",
    "        print('first subtree of constituency parse')\n",
    "        print(constituency_parse.child[0])\n",
    "\n",
    "        # get the value of the first subtree\n",
    "        print('---')\n",
    "        print('value of first subtree of constituency parse')\n",
    "        print(constituency_parse.child[0].value)\n",
    "\n",
    "        # get the dependency parse of the first sentence\n",
    "        print('---')\n",
    "        print('dependency parse of first sentence')\n",
    "        dependency_parse = sentence.basicDependencies\n",
    "        print(dependency_parse)\n",
    "\n",
    "        # get the first token of the first sentence\n",
    "        print('---')\n",
    "        print('first token of first sentence')\n",
    "        token = sentence.token[0]\n",
    "        print(token)\n",
    "\n",
    "        # get the part-of-speech tag\n",
    "        print('---')\n",
    "        print('part of speech tag of token')\n",
    "        token.pos\n",
    "        print(token.pos)\n",
    "\n",
    "#         # get the named entity tag\n",
    "#         print('---')\n",
    "#         print('named entity tag of token')\n",
    "#         print(token.ner)\n",
    "\n",
    "#         # get an entity mention from the first sentence\n",
    "#         print('---')\n",
    "#         print('first entity mention in sentence')\n",
    "#         print(sentence.mentions[0])\n",
    "\n",
    "#         # access the coref chain\n",
    "#         print('---')\n",
    "#         print('coref chains for the example')\n",
    "#         print(ann.corefChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
