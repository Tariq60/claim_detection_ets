{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "from termcolor import colored\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wm_tool_eval(gold_sent_labels, wm_sent_labels):\n",
    "    '''\n",
    "    gold_sent_labels = list of sentences where each sentence is a list of gold labels of its tokens\n",
    "    labels should be in 'B-claim', 'I-claim', 'O-claim'\n",
    "    \n",
    "    wm_sent_labels = list of sentences where each sentence is a list of prediction labels of its tokens (WM tool)\n",
    "    labels should be in 'claim', 'no-claim'\n",
    "    '''\n",
    "    # total number of claim and no-claim segments in all sentences, with max of 1 no-claim segment per sentence\n",
    "    claims_segment_count, claims_correct, claims_mistake = 0, 0, 0\n",
    "    noclaims_segment_count, noclaims_correct, noclaims_mistake = 0, 0, 0\n",
    "    \n",
    "    # looping through all sentences\n",
    "    for gold_sent, wm_tagged_sent in zip(gold_sent_labels, wm_sent_labels):\n",
    "        no_claim_tokens, all_claim_tokens, single_claim_tokens = [], [], []\n",
    "        \n",
    "        # looping through token_labels for a single sentence\n",
    "        for gold_token, wm_tagged_token in zip(gold_sent, wm_tagged_sent):\n",
    "            \n",
    "            if gold_token == 'O-claim':\n",
    "                no_claim_tokens.append(wm_tagged_token)\n",
    "            \n",
    "            else:\n",
    "                if gold_token == 'B-claim':\n",
    "                    if len(single_claim_tokens) == 0:\n",
    "                        single_claim_tokens.append(wm_tagged_token)\n",
    "                    else:\n",
    "                        all_claim_tokens.append(single_claim_tokens)\n",
    "                        single_claim_tokens = []\n",
    "                \n",
    "                else: #gold_token == 'I-claim'\n",
    "                    single_claim_tokens.append(wm_tagged_token)\n",
    "        \n",
    "        # adding the last claim in the sentence\n",
    "        if len(single_claim_tokens) > 0:\n",
    "            all_claim_tokens.append(single_claim_tokens)\n",
    "                        \n",
    "        \n",
    "        # checking if all of the 'O-claim' tokens are labels as 'no-claim' by the WM tool\n",
    "        if len(no_claim_tokens) > 0:\n",
    "            noclaims_segment_count += 1\n",
    "            if all([label == 'no-claim' for label in no_claim_tokens]):\n",
    "                noclaims_correct += 1\n",
    "            else:\n",
    "                noclaims_mistake += 1\n",
    "                \n",
    "        # for each claim segment in the sentence\n",
    "        #     checking that it has at least token tagged as 'claim' by the WM tool\n",
    "        for claim in all_claim_tokens:\n",
    "            claims_segment_count += 1\n",
    "            if any([label == 'claim' for label in claim]):\n",
    "                claims_correct += 1\n",
    "            else:\n",
    "                claims_mistake += 1\n",
    "    \n",
    "    # returning the total number of segments with the counts of correct and wrong tagged segments by the WM tool\n",
    "    return noclaims_segment_count, noclaims_correct, noclaims_mistake, claims_segment_count, claims_correct, claims_mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 0, 3, 2, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm_tool_eval([['O-claim', 'O-claim', 'O-claim', 'O-claim', 'O-claim', 'O-claim'], \n",
    "              ['B-claim', 'I-claim', 'I-claim', 'O-claim', 'B-claim', 'I-claim'],\n",
    "              ['O-claim', 'B-claim', 'I-claim', 'I-claim', 'I-claim', 'O-claim']],\n",
    "             \n",
    "             [['no-claim', 'no-claim', 'no-claim', 'no-claim', 'no-claim', 'no-claim'], \n",
    "              ['no-claim', 'claim', 'no-claim', 'no-claim', 'no-claim', 'no-claim'], \n",
    "              ['no-claim', 'no-claim', 'claim', 'no-claim', 'no-claim', 'no-claim']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wm_essays(essays_dir, header=False):\n",
    "     # read files\n",
    "    data = []\n",
    "    for file in sorted(glob.glob(essays_dir+'*.tsv')):\n",
    "        data.append(open(file).readlines())\n",
    "\n",
    "\n",
    "    essays_sent_token_label, tokens, labels = [], [], []\n",
    "\n",
    "    for essay_id, essay in enumerate(data):\n",
    "        prev_sent_id = '0'\n",
    "        essay_sents, sent_token, sent_label = [], [], []\n",
    "        doc_tokens, doc_labels = [], []\n",
    "        \n",
    "        if header:\n",
    "            essay = essay[1:]\n",
    "        \n",
    "        for i,line in enumerate(essay):\n",
    "            if '_NEW_LINE_' not in line:\n",
    "                sent_id, token_id, token, label = line.rstrip().split()\n",
    "\n",
    "                if sent_id != prev_sent_id:\n",
    "                    essay_sents.append((sent_token, sent_label))\n",
    "                    sent_token, sent_label = [], []\n",
    "\n",
    "                if len(token) < 25 and 'www' not in token:\n",
    "                    doc_tokens.append(token)\n",
    "                    doc_labels.append('{}-claim'.format(label.split('-')[0]))\n",
    "                    sent_token.append(token)\n",
    "                    sent_label.append('{}-claim'.format(label.split('-')[0]))\n",
    "\n",
    "                prev_sent_id = sent_id\n",
    "        \n",
    "        essay_sents.append((sent_token, sent_label))\n",
    "        essays_sent_token_label.append(essay_sents)\n",
    "        tokens.append(doc_tokens)\n",
    "        labels.append(doc_labels)\n",
    "\n",
    "    essay_str, essay_str_sent = [], []\n",
    "    for essay in essays_sent_token_label:\n",
    "        \n",
    "        sentences = []\n",
    "        for essay_sent_tokens, essay_sent_labels in essay:\n",
    "            sent = ' '.join(essay_sent_tokens)\n",
    "    #         sent = sent.replace(\" ' m\", \"'m\")\n",
    "    #         sent = sent.replace(\" ' s\", \"'s\")\n",
    "    #         sent = sent.replace(\" : \", \": \")\n",
    "            sentences.append(sent)\n",
    "        \n",
    "        essay_str_sent.append(sentences)\n",
    "        essay_str.append(' '.join(sentences))\n",
    "\n",
    "    return {'essay_sent_token_label': essays_sent_token_label, 'tokens': tokens, 'labels': labels,\n",
    "            'essay': essay_str, 'essay_sent': essay_str_sent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm1 = read_wm_essays('/Users/talhindi/Documents/data_wm/arg_clean_45_1/')\n",
    "wm2 = read_wm_essays('/Users/talhindi/Documents/data_wm/arg_clean_45_2/')\n",
    "wm_nr = read_wm_essays('/Users/talhindi/Documents/data_wm/wm_narrative/', header=True)\n",
    "sg = read_wm_essays('/Users/talhindi/Documents/claim_detection/data/SG2017_claim/train/', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_sent = [sent for essay_sent in wm2['essay_sent'] for sent in essay_sent]\n",
    "sg_sent = [sent for essay_sent in sg['essay_sent'] for sent in essay_sent]\n",
    "wm_nr_sent = [sent for essay_sent in wm_nr['essay_sent'] for sent in essay_sent]\n",
    "\n",
    "random.seed(2453)\n",
    "random.shuffle(wm_sent)\n",
    "random.shuffle(sg_sent)\n",
    "random.shuffle(wm_nr_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SG vs WM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5586"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_sent = []\n",
    "for sent1, sent2 in zip(wm_sent, sg_sent[:len(wm_sent)]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "for sent1, sent2 in zip(wm_sent, sg_sent[len(wm_sent):2*len(wm_sent)]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "for sent1, sent2 in zip(wm_sent, sg_sent[2*len(wm_sent):3*len(wm_sent)]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "len(mixed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3721, 11273)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm_same_sent = []\n",
    "for i in range(1,len(wm_sent)):\n",
    "    wm_same_sent.append('{}\\t{}\\t1\\n'.format(wm_sent[i-1], wm_sent[i]))\n",
    "    if i-2 >= 0:\n",
    "        wm_same_sent.append('{}\\t{}\\t1\\n'.format(wm_sent[i-1], wm_sent[i]))\n",
    "\n",
    "sg_same_sent = []\n",
    "for i in range(1,len(sg_sent)-1):\n",
    "    sg_same_sent.append('{}\\t{}\\t1\\n'.format(sg_sent[i-1], sg_sent[i]))\n",
    "    if i-2 >= 0:\n",
    "        sg_same_sent.append('{}\\t{}\\t1\\n'.format(sg_sent[i-1], sg_sent[i]))\n",
    "\n",
    "random.seed(2453)\n",
    "random.shuffle(wm_same_sent)\n",
    "random.shuffle(sg_same_sent)\n",
    "\n",
    "len(wm_same_sent), len(sg_same_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent = mixed_sent[:5000] + wm_same_sent[:2500] + sg_same_sent[:2500]\n",
    "random.seed(2453)\n",
    "random.shuffle(all_sent)\n",
    "print(len(all_sent))\n",
    "\n",
    "with open('/Users/talhindi/Documents/claim_detection/data/sg_wm_sent.tsv', 'w') as writer:\n",
    "    writer.writelines(all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/talhindi/Documents/claim_detection/data/sg_wm_sent.tsv', 'w') as writer:\n",
    "    writer.writelines(all_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WM-arg VS WM-nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5556"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_sent = []\n",
    "for sent1, sent2 in zip(wm_nr_sent, wm_sent[:len(wm_nr_sent)]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "for sent1, sent2 in zip(wm_nr_sent[:len(wm_sent)-len(wm_nr_sent)], wm_sent[len(wm_nr_sent):]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "#shift by 5       \n",
    "for sent1, sent2 in zip(wm_nr_sent, wm_sent[5:len(wm_nr_sent)+5]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "for sent1, sent2 in zip(wm_nr_sent[:len(wm_sent)-len(wm_nr_sent)], wm_sent[len(wm_nr_sent)+5:-5]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "#shift by 10\n",
    "for sent1, sent2 in zip(wm_nr_sent, wm_sent[10:len(wm_nr_sent)+10]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "for sent1, sent2 in zip(wm_nr_sent[:len(wm_sent)-len(wm_nr_sent)], wm_sent[len(wm_nr_sent)+10:-10]):\n",
    "    if int(10*random.random()) % 2 == 0:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent1, sent2))\n",
    "    else:\n",
    "        mixed_sent.append('{}\\t{}\\t0\\n'.format(sent2, sent1))\n",
    "\n",
    "\n",
    "len(mixed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3721, 2659)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmarg_same_sent = []\n",
    "for i in range(1,len(wm_sent)):\n",
    "    wmarg_same_sent.append('{}\\t{}\\t1\\n'.format(wm_sent[i-1], wm_sent[i]))\n",
    "    if i-2 >= 0:\n",
    "        wmarg_same_sent.append('{}\\t{}\\t1\\n'.format(wm_sent[i-1], wm_sent[i]))\n",
    "\n",
    "wmnr_same_sent = []\n",
    "for i in range(1,len(wm_nr_sent)-1):\n",
    "    wmnr_same_sent.append('{}\\t{}\\t1\\n'.format(wm_nr_sent[i-1], wm_nr_sent[i]))\n",
    "    if i-2 >= 0:\n",
    "        wmnr_same_sent.append('{}\\t{}\\t1\\n'.format(wm_nr_sent[i-1], wm_nr_sent[i]))\n",
    "\n",
    "random.seed(2453)\n",
    "random.shuffle(wmarg_same_sent)\n",
    "random.shuffle(wmnr_same_sent)\n",
    "\n",
    "len(wmarg_same_sent), len(wmnr_same_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "all_sent = mixed_sent[:5000] + wmarg_same_sent[:2500] + wmnr_same_sent[:2500]\n",
    "random.seed(2453)\n",
    "random.shuffle(all_sent)\n",
    "print(len(all_sent))\n",
    "\n",
    "with open('/Users/talhindi/Documents/claim_detection/data/wmarg_wmnr_sent.tsv', 'w') as writer:\n",
    "    writer.writelines(all_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
