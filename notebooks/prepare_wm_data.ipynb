{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_test_file(files_dir, export=True):\n",
    "    data = []\n",
    "    for i, file in enumerate(sorted(glob.glob(files_dir+'*.tsv'))):\n",
    "#         print(i, file)\n",
    "        if 'test.txt' not in file:\n",
    "            data.append(open(file).readlines())\n",
    "    \n",
    "    test_set, count = [], 0\n",
    "\n",
    "    for essay_id,essay in enumerate(data):\n",
    "#         print(essay_id)\n",
    "        prev_sent_id = '0'\n",
    "\n",
    "        for line in essay[1:]:\n",
    "            sent_id, token_id, token, label = line.rstrip().split('\\t')\n",
    "\n",
    "            if sent_id != prev_sent_id:\n",
    "                test_set.append('\\n')\n",
    "                count += 1\n",
    "\n",
    "            if len(token) < 25 and 'www' not in token and '_NEW_LINE_' not in token:\n",
    "                test_set.append('{} {}-claim\\n'.format(token, label.split('-')[0]))\n",
    "\n",
    "            prev_sent_id = sent_id\n",
    "\n",
    "        test_set.append('\\n')\n",
    "        count += 1\n",
    "        \n",
    "    print(len(data), len(test_set), len(test_set)-count)\n",
    "\n",
    "    if export:\n",
    "        with open(files_dir+'test.txt', 'w') as writer:\n",
    "            for line in test_set:\n",
    "                writer.write(line)\n",
    "    \n",
    "    return data, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exporting merged files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _ = export_test_file('/Users/talhindi/Documents/data_wm/raw_data/',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 26176 24910\n"
     ]
    }
   ],
   "source": [
    "_, _ = export_test_file('/Users/talhindi/Documents/data_wm/arg_clean_45_1/',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 38408 36546\n"
     ]
    }
   ],
   "source": [
    "export_test_file('/Users/talhindi/Documents/data_wm/arg_clean_45_2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 23184 21852\n"
     ]
    }
   ],
   "source": [
    "export_test_file('/Users/talhindi/Documents/data_wm/wm_narrative/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291 111582 106484\n"
     ]
    }
   ],
   "source": [
    "export_test_file('/Users/talhindi/Documents/claim_detection/data_newsplit/debanjan/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 12260 11719\n"
     ]
    }
   ],
   "source": [
    "export_test_file('/Users/talhindi/Documents/claim_detection/data_newsplit/debanjan/valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 26221 24955\n"
     ]
    }
   ],
   "source": [
    "export_test_file('/Users/talhindi/Documents/claim_detection/data_newsplit/debanjan/eval/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking examples for the presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wm_essays(essays_dir):\n",
    "     # read files\n",
    "    data = []\n",
    "    for file in sorted(glob.glob(essays_dir+'*.tsv')):\n",
    "        data.append(open(file).readlines())\n",
    "\n",
    "\n",
    "    essays_sent_token_label, tokens, labels = [], [], []\n",
    "\n",
    "    for essay in data:\n",
    "        prev_sent_id = '0'\n",
    "        essay_sents, sent_token, sent_label = [], [], []\n",
    "        doc_tokens, doc_labels = [], []\n",
    "\n",
    "        for line in essay:\n",
    "            sent_id, token_id, token, label = line.rstrip().split()\n",
    "            \n",
    "            if sent_id != prev_sent_id:\n",
    "                essay_sents.append((sent_token, sent_label))\n",
    "                sent_token, sent_label = [], []\n",
    "                \n",
    "            if len(token) < 25 and 'www' not in token:\n",
    "                doc_tokens.append(token)\n",
    "                doc_labels.append('{}-claim'.format(label.split('-')[0]))\n",
    "                sent_token.append(token)\n",
    "                sent_label.append('{}-claim'.format(label.split('-')[0]))\n",
    "            \n",
    "            prev_sent_id = sent_id\n",
    "        \n",
    "        essay_sents.append((sent_token, sent_label))\n",
    "        essays_sent_token_label.append(essay_sents)\n",
    "        tokens.append(doc_tokens)\n",
    "        labels.append(doc_labels)\n",
    "\n",
    "    essay_str, essay_str_sent = [], []\n",
    "    for essay in essays_sent_token_label:\n",
    "        \n",
    "        sentences = []\n",
    "        for essay_sent_tokens, essay_sent_labels in essay:\n",
    "            sent = ' '.join(essay_sent_tokens)\n",
    "    #         sent = sent.replace(\" ' m\", \"'m\")\n",
    "    #         sent = sent.replace(\" ' s\", \"'s\")\n",
    "    #         sent = sent.replace(\" : \", \": \")\n",
    "            sentences.append(sent)\n",
    "        \n",
    "        essay_str_sent.append(sentences)\n",
    "        essay_str.append(' '.join(sentences))\n",
    "\n",
    "    return {'essay_sent_token_label': essays_sent_token_label, 'tokens': tokens, 'labels': labels,\n",
    "            'essay': essay_str, 'essay_sent': essay_str_sent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm1 = read_wm_essays('/Users/talhindi/Documents/data_wm/arg_clean_45_1/')\n",
    "wm2 = read_wm_essays('/Users/talhindi/Documents/data_wm/arg_clean_45_2/')\n",
    "wm_nr = read_wm_essays('/Users/talhindi/Documents/data_wm/wm_narrative/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-2786\t1-2637\t2-3353\t3-5440\t\u001b[31m4-1813\u001b[0m\t5-2719\t6-2301\t7-2898\t\u001b[31m8-1770\u001b[0m\t9-2984\t\n",
      "10-4182\t11-3838\t12-2339\t13-3240\t14-4158\t15-3108\t16-3376\t17-2561\t\u001b[31m18-1469\u001b[0m\t\u001b[31m19-1088\u001b[0m\t\n",
      "20-2943\t21-2104\t22-3744\t23-2743\t\u001b[31m24-834\u001b[0m\t25-2133\t26-4147\t27-3777\t28-3539\t29-3434\t\n",
      "30-4017\t31-2369\t32-4118\t33-3177\t34-4005\t\u001b[31m35-1853\u001b[0m\t\u001b[31m36-1776\u001b[0m\t37-2656\t38-3219\t39-2538\t\n",
      "40-2354\t41-4821\t42-2436\t43-2491\t44-2248\t\n",
      "\n",
      "0-6597\t\u001b[31m1-304\u001b[0m\t\u001b[31m2-640\u001b[0m\t\u001b[31m3-794\u001b[0m\t4-2515\t\u001b[31m5-1771\u001b[0m\t6-6962\t7-3903\t8-4167\t\u001b[31m9-809\u001b[0m\t\n",
      "\u001b[31m10-1061\u001b[0m\t\u001b[31m11-147\u001b[0m\t\u001b[31m12-449\u001b[0m\t13-2003\t14-5468\t\u001b[31m15-117\u001b[0m\t16-2890\t17-5024\t18-4277\t19-18384\t\n",
      "\u001b[31m20-1796\u001b[0m\t21-4192\t\u001b[31m22-1018\u001b[0m\t23-2644\t24-4747\t25-2582\t\u001b[31m26-1601\u001b[0m\t27-6179\t28-4151\t29-5981\t\n"
     ]
    }
   ],
   "source": [
    "for i,essay in enumerate(wm1['essay']):\n",
    "    if len(essay) < 2000:\n",
    "        print(colored('{}-{}'.format(i,len(essay)), 'red'), end='\\t')\n",
    "    else:\n",
    "        print('{}-{}'.format(i,len(essay)), end='\\t')\n",
    "    if (i+1) % 10 == 0:\n",
    "        print()\n",
    "\n",
    "print('\\n')\n",
    "for i,essay in enumerate(wm_nr['essay']):\n",
    "    if len(essay) < 2000:\n",
    "        print(colored('{}-{}'.format(i,len(essay)), 'red'), end='\\t')\n",
    "    else:\n",
    "        print('{}-{}'.format(i,len(essay)), end='\\t')\n",
    "    if (i+1) % 10 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wm1['essay_sent'][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 10 13 15 24 25 28 29 "
     ]
    }
   ],
   "source": [
    "for i,labels in enumerate(wm_nr['labels']):\n",
    "    if 'B-claim' in labels:\n",
    "        print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' token Overcoming Moving Schools and Homes According to the U.S. Census Bureau nearly 36 million people move every year in the U.S. - and , as a result , many children have to transfer schools , the Statesman Journal said in an article on tips for families with students transferring schools . That is about eleven percent of the United States population that moves per year . Moving schools can be fun and exciting or scary and sad , and at the same time being all of that . Especially when put on top of general house moving , too . The months before the realtor staked the For Sale sign in the crest of the ditch in our grassy green front yard , the spot where Kendall , Tyler , and Quinn had often marked as third base while playing baseball , they were tough months , on the whole family .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm_nr['essay'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should DNA modification be legal ? \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "So DNA modification it sounds cool right . \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "DNA modification is harmful . \n",
      "B-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "Most people believe that DNA modification started in China . \n",
      "O-claim O-claim O-claim O-claim B-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "But DNA modification did not start in China . \n",
      "B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "DNA modification should be illegal , and here is why . \n",
      "B-claim I-claim I-claim I-claim I-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "DNA modification is dangerous . \n",
      "B-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "If a person messes up a DNA strand , it could kill a child . \n",
      "O-claim B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "Would you want somebody to genetically modify your child ? \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "Sure DNA modification can do what most anti biotics can not but with a higher risk of death . \n",
      "B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "Is the greater risk of death worth it ? \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "DNA modification is too dangerous . \n",
      "B-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "And diseases that we do not know of . \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "If people wanted to modifiy an unborn child then they would have to work on the egg itself . \n",
      "O-claim B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "So if you wanted the change to affect the child . \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "So if you were a mother or father and your kid has diabetes that is genetic . \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "Doctors do not use DNA modification . \n",
      "B-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "This is because DNA modification is unpredictable and life threatining . \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "So if you make a clone you would most likely have to go to court if found by police . \n",
      "O-claim O-claim B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "I mean sure in this case if they use DNA modification to make clones they could save peoples lives from deadly injuries . \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "And life expectency would rise dramaticaly . \n",
      "O-claim B-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "We could also use DNA modification to heal deep wounds by making the human healing rate faster . \n",
      "B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "For example , if people use DNA modification afterward we could make soldiers stronger and people immune to more sicknesses . \n",
      "O-claim O-claim O-claim O-claim B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "Sure you can edit DNA and most likely get rid of some allergies and or sicknesses that are in genetics although only when young or in the womb . \n",
      "O-claim B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n",
      "Accordingly learned that DNA modification can be unpredictable . \n",
      "O-claim O-claim O-claim B-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "These are only a couple of reasons that DNA modification should be illegal . \n",
      "B-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim I-claim O-claim \n",
      "\n",
      "Would you want you child to be modified ? \n",
      "O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim O-claim \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent_tokens, sent_labels in wm2['essay_sent_token_label'][24]:\n",
    "    for token in sent_tokens: print(token, end=' ')\n",
    "    print()\n",
    "    for label in sent_labels: print(label, end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 198)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = open('/Users/talhindi/Documents/claim_detection_wm/src/token_ids.txt').readlines()\n",
    "LR_pred = pd.read_csv('/Users/talhindi/Documents/claim_detection_wm/skll/skll_wm_wm-arg/predictions/one_essay.tsv', sep='\\t', header=None)\n",
    "len(tokens), len(LR_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mB-claim\u001b[0m There\n",
      "\u001b[34mI-claim\u001b[0m must\n",
      "\u001b[34mI-claim\u001b[0m be\n",
      "\u001b[34mI-claim\u001b[0m stricter\n",
      "O-claim punishments\n",
      "O-claim to\n",
      "\u001b[34mI-claim\u001b[0m countries\n",
      "O-claim or\n",
      "O-claim people\n",
      "O-claim who\n",
      "O-claim break\n",
      "\u001b[34mI-claim\u001b[0m them\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m War\n",
      "O-claim is\n",
      "O-claim always\n",
      "O-claim brutal\n",
      "O-claim ,\n",
      "O-claim but\n",
      "\u001b[32mB-claim\u001b[0m this\n",
      "\u001b[34mI-claim\u001b[0m will\n",
      "\u001b[34mI-claim\u001b[0m at\n",
      "O-claim least\n",
      "O-claim reduce\n",
      "O-claim the\n",
      "\u001b[34mI-claim\u001b[0m number\n",
      "\u001b[34mI-claim\u001b[0m of\n",
      "\u001b[34mI-claim\u001b[0m casualties\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m If\n",
      "\u001b[32mB-claim\u001b[0m we\n",
      "\u001b[34mI-claim\u001b[0m could\n",
      "\u001b[34mI-claim\u001b[0m prevent\n",
      "O-claim war\n",
      "O-claim crimes\n",
      "O-claim ,\n",
      "O-claim then\n",
      "\u001b[32mB-claim\u001b[0m many\n",
      "\u001b[34mI-claim\u001b[0m more\n",
      "\u001b[34mI-claim\u001b[0m people\n",
      "\u001b[34mI-claim\u001b[0m would\n",
      "\u001b[34mI-claim\u001b[0m live\n",
      "O-claim through\n",
      "\u001b[34mI-claim\u001b[0m a\n",
      "O-claim conflict\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m Specific\n",
      "O-claim weapons\n",
      "O-claim being\n",
      "O-claim banned\n",
      "O-claim for\n",
      "O-claim reasons\n",
      "O-claim like\n",
      "O-claim massive\n",
      "O-claim property\n",
      "O-claim damage\n",
      "O-claim ,\n",
      "O-claim unhuman\n",
      "O-claim deaths\n",
      "O-claim ,\n",
      "O-claim can\n",
      "\u001b[34mI-claim\u001b[0m seriously\n",
      "O-claim injure\n",
      "O-claim someone\n",
      "O-claim ,\n",
      "O-claim uncontainable\n",
      "O-claim ,\n",
      "O-claim and\n",
      "O-claim uncontrollable\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m The\n",
      "\u001b[34mI-claim\u001b[0m land\n",
      "\u001b[34mI-claim\u001b[0m will\n",
      "\u001b[34mI-claim\u001b[0m suffer\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m A\n",
      "O-claim good\n",
      "O-claim example\n",
      "O-claim of\n",
      "O-claim the\n",
      "O-claim land\n",
      "\u001b[34mI-claim\u001b[0m suffer\n",
      "\u001b[34mI-claim\u001b[0m in\n",
      "\u001b[34mI-claim\u001b[0m a\n",
      "\u001b[34mI-claim\u001b[0m war\n",
      "O-claim is\n",
      "\u001b[34mI-claim\u001b[0m agent\n",
      "O-claim orange\n",
      "O-claim during\n",
      "O-claim Vietnam\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m Agent\n",
      "O-claim Orange\n",
      "O-claim has\n",
      "O-claim had\n",
      "O-claim adverse\n",
      "\u001b[34mI-claim\u001b[0m effects\n",
      "\u001b[34mI-claim\u001b[0m on\n",
      "O-claim Vietnam\n",
      "O-claim 's\n",
      "\u001b[34mI-claim\u001b[0m foliage\n",
      "\u001b[34mI-claim\u001b[0m and\n",
      "\u001b[34mI-claim\u001b[0m animal\n",
      "\u001b[34mI-claim\u001b[0m life\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m Dioxin\n",
      "O-claim is\n",
      "O-claim a\n",
      "O-claim highly\n",
      "O-claim persistent\n",
      "O-claim chemical\n",
      "O-claim compound\n",
      "O-claim that\n",
      "O-claim lasts\n",
      "O-claim for\n",
      "O-claim many\n",
      "O-claim years\n",
      "O-claim in\n",
      "O-claim the\n",
      "O-claim environment\n",
      "O-claim ,\n",
      "O-claim particularly\n",
      "O-claim in\n",
      "O-claim soil\n",
      "O-claim ,\n",
      "O-claim lake\n",
      "O-claim and\n",
      "O-claim river\n",
      "\u001b[34mI-claim\u001b[0m sediments\n",
      "O-claim and\n",
      "O-claim in\n",
      "O-claim the\n",
      "O-claim food\n",
      "O-claim chain\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m 60\n",
      "O-claim %\n",
      "O-claim of\n",
      "\u001b[34mI-claim\u001b[0m deaths\n",
      "\u001b[34mI-claim\u001b[0m in\n",
      "O-claim WW2\n",
      "O-claim were\n",
      "\u001b[34mI-claim\u001b[0m civilians\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m Civilian\n",
      "O-claim deaths\n",
      "O-claim totaled\n",
      "O-claim 50\n",
      "O-claim -\n",
      "O-claim 55\n",
      "O-claim million\n",
      "\u001b[34mI-claim\u001b[0m in\n",
      "O-claim WW2\n",
      "O-claim .\n",
      "O-claim World\n",
      "O-claim War\n",
      "O-claim II\n",
      "O-claim Casualties\n",
      "O-claim .\n",
      "O-claim In\n",
      "O-claim Germany\n",
      "O-claim alone\n",
      "O-claim had\n",
      "O-claim over\n",
      "O-claim 8,680,000\n",
      "O-claim civilian\n",
      "\u001b[34mI-claim\u001b[0m deaths\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m Digging\n",
      "O-claim trenches\n",
      "O-claim caused\n",
      "O-claim trampling\n",
      "O-claim of\n",
      "O-claim grassland\n",
      "O-claim ,\n",
      "O-claim crushing\n",
      "O-claim of\n",
      "\u001b[34mI-claim\u001b[0m plants\n",
      "O-claim and\n",
      "O-claim animals\n",
      "O-claim ,\n",
      "O-claim and\n",
      "\u001b[34mI-claim\u001b[0m churning\n",
      "\u001b[34mI-claim\u001b[0m of\n",
      "\u001b[34mI-claim\u001b[0m soil\n",
      "O-claim .\n",
      "\u001b[32mB-claim\u001b[0m Erosion\n",
      "O-claim resulted\n",
      "O-claim from\n",
      "O-claim forest\n",
      "O-claim logging\n",
      "\u001b[34mI-claim\u001b[0m to\n",
      "\u001b[34mI-claim\u001b[0m expand\n",
      "\u001b[34mI-claim\u001b[0m the\n",
      "\u001b[34mI-claim\u001b[0m network\n",
      "\u001b[34mI-claim\u001b[0m of\n",
      "\u001b[34mI-claim\u001b[0m trenches\n",
      "O-claim .\n"
     ]
    }
   ],
   "source": [
    "i, labels = 0, {1: 'B-claim', 2:'I-claim', 0:'O-claim'}\n",
    "for row in LR_pred.iterrows():\n",
    "    color_id = np.argmax(row[1][1:])\n",
    "    if color_id == 1:\n",
    "        print(colored(labels[np.argmax(row[1][1:])],'green'), tokens[i].split()[0])\n",
    "    elif color_id == 2:\n",
    "        print(colored(labels[np.argmax(row[1][1:])],'blue'), tokens[i].split()[0])\n",
    "    else:\n",
    "        print(labels[np.argmax(row[1][1:])], tokens[i].split()[0])\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_sent_token_label, tokens, labels = [], [], []\n",
    "\n",
    "for essay in data:\n",
    "    prev_sent_id = '0'\n",
    "    essay_sents, sent_token, sent_label = [], [], []\n",
    "    doc_tokens, doc_labels = [], []\n",
    "\n",
    "    for line in essay:\n",
    "        sent_id, token_id, token, label = line.rstrip().split()\n",
    "        \n",
    "        if sent_id != prev_sent_id:\n",
    "            essay_sents.append((sent_token, sent_label))\n",
    "            sent_token, sent_label = [], []\n",
    "            \n",
    "        if len(token) < 25 and 'www' not in token:\n",
    "            doc_tokens.append(token)\n",
    "            doc_labels.append(label)\n",
    "            sent_token.append(token)\n",
    "            sent_label.append('{}-claim\\n'.format(label.split('-')[0]))\n",
    "        \n",
    "        prev_sent_id = sent_id\n",
    "    \n",
    "    essay_sents.append((sent_token, sent_label))\n",
    "    essays_sent_token_label.append(essay_sents)\n",
    "    tokens.append(doc_tokens)\n",
    "    labels.append(doc_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 27134)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(essays_sent_token_label),sum([len(s[0]) for e in essays_sent_token_label for s in e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27134, 27134)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(doc_tokens) for doc_tokens in tokens]), sum([len(doc_labels) for doc_labels in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_str, essay_str_sent = [], []\n",
    "for essay in essays_sent_token_label:\n",
    "    \n",
    "    sentences = []\n",
    "    for essay_sent_tokens, essay_sent_labels in essay:\n",
    "        sent = ' '.join(essay_sent_tokens)\n",
    "#         sent = sent.replace(\" ' m\", \"'m\")\n",
    "#         sent = sent.replace(\" ' s\", \"'s\")\n",
    "#         sent = sent.replace(\" : \", \": \")\n",
    "        sentences.append(sent)\n",
    "    \n",
    "    essay_str_sent.append(sentences)\n",
    "    essay_str.append(' '.join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_spacy = []\n",
    "for essay in essay_str:\n",
    "    essay_spacy.append(nlp(essay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for essay_tok, essay_org, essay_sent in zip(essay_spacy,essays_sent_token_label, essay_str_sent):\n",
    "    \n",
    "    # asserting number of spacy tokens is equal to the original number of tokens\n",
    "    assert len(essay_tok) == sum([len(sent_token) for sent_token, sent_label in essay_org])\n",
    "    \n",
    "#     count_sent = 0\n",
    "#     for i, sent in enumerate(essay_tok.sents):\n",
    "#         print(sent,'\\n', essay_sent[max(0,i-1)], '\\n')\n",
    "#         count_sent += 1\n",
    "#     print(count_sent, len(essay_org))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "Take the most feared shark of them all the great white : From 1926 - 2011 great whites attacked Americans one hundred and six times when unprovoked and only thirteen of those attacks proved fatal . \n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "# tmp = nlp(\"You know what I ' m talking about the large fish that can kill you with one advance at your limbs .\")\n",
    "tmp = nlp(\"Take the most feared shark of them all the great white : From 1926 - 2011 great whites attacked Americans one hundred and six times when unprovoked and only thirteen of those attacks proved fatal .\")\n",
    "# tmp = nlp(\"Not only are the sharks eating their fellow brothers ' and sisters ' bodies but also the unfertilized eggs in a fit of embryonic cannibalism .\")\n",
    "\n",
    "print(len(tmp))\n",
    "for sent in tmp.sents:\n",
    "    print(sent, '\\n*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = nlp(\"You know what I'm talking about the large fish that can kill you with one advance at your limbs .\")\n",
    "# tmp = nlp(\"Take the most feared shark of them all the great white: From 1926 - 2011 great whites attacked Americans one hundred and six times when unprovoked and only thirteen of those attacks proved fatal .\")\n",
    "# tmp = nlp(\"Not only are the sharks eating their fellow brothers' and sisters' bodies but also the unfertilized eggs in a fit of embryonic cannibalism .\")\n",
    "\n",
    "print(len(tmp))\n",
    "for sent in tmp.sents:\n",
    "    print(sent, '\\n*****')\n",
    "for token in tmp:\n",
    "    print('*{}*'.format(token), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"You know what I ' m talking about the large fish that can kill you with one advance at your limbs .\".find(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You know what I' m talking about the large fish that can kill you with one advance at your limbs .\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"You know what I ' m talking about the large fish that can kill you with one advance at your limbs .\".replace(\" '\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
