{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from merged essay files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_sent_labels(file):\n",
    "    \n",
    "    labeled_sentences = []\n",
    "    sent, sent_token_labels = '', []\n",
    "    \n",
    "    for i, line in enumerate(file):\n",
    "        if line.rstrip() != '':\n",
    "            try:\n",
    "                token, label = line.rstrip().split()\n",
    "            except:\n",
    "                print(i, line)\n",
    "            sent += token + ' '\n",
    "            sent_token_labels.append(label)\n",
    "        else:\n",
    "            sent_label = any([label in ['Arg-I', 'I-claim', 'I-premise'] for label in sent_token_labels])\n",
    "            if sent_label:\n",
    "                labeled_sentences.append((sent.strip(), 1))\n",
    "            else:\n",
    "                labeled_sentences.append((sent.strip(), 0))\n",
    "            sent, sent_token_labels = '', []\n",
    "    \n",
    "    return labeled_sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "path = '../data/SG2017_tok/'\n",
    "train = open(os.path.join(path,'train.txt')).readlines()\n",
    "test = open(os.path.join(path,'test.txt')).readlines()\n",
    "\n",
    "print('train')\n",
    "train_sent = token_to_sent_labels(train)\n",
    "print('test')\n",
    "test_sent = token_to_sent_labels(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "path = '../data/SG2017_claim/'\n",
    "train = open(os.path.join(path,'train.txt')).readlines()\n",
    "test = open(os.path.join(path,'test.txt')).readlines()\n",
    "\n",
    "print('train')\n",
    "train_sent = token_to_sent_labels(train)\n",
    "print('test')\n",
    "test_sent = token_to_sent_labels(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from separated essay files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_list_token_to_essay_list_sent(file_list):\n",
    "    \n",
    "    essay_labeled_sentences, labeled_sentences = [], []\n",
    "    sent, sent_token_labels = '', []\n",
    "    \n",
    "    for file_id, file in enumerate(file_list):\n",
    "        \n",
    "        # first sentence id\n",
    "        prev_sent_id = '0'\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            try:\n",
    "                sent_id, token_id, token, label = line.rstrip().split('\\t')\n",
    "            except:\n",
    "                print(file_id, i, line.rstrip().split('\\t'))\n",
    "            \n",
    "            if sent_id == prev_sent_id:\n",
    "                if token not in ['_NEW_LINE__NEW_LINE_', '_NEW_LINE_']:\n",
    "                    sent += token + ' '\n",
    "                    sent_token_labels.append(label)\n",
    "                prev_sent_id = sent_id\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                sent_label = any([label in ['Arg-I', 'I-claim', 'I-premise'] for label in sent_token_labels])\n",
    "                if sent_label:\n",
    "                    essay_labeled_sentences.append((sent.strip(), 1))\n",
    "                else:\n",
    "                    essay_labeled_sentences.append((sent.strip(), 0))\n",
    "                \n",
    "                # initializing with first token of the next sentence\n",
    "                sent, sent_token_labels = token+' ', [label]\n",
    "                prev_sent_id = sent_id\n",
    "        \n",
    "        \n",
    "        sent_label = any([label in ['Arg-I', 'I-claim', 'I-premise'] for label in sent_token_labels])\n",
    "        if sent_label:\n",
    "            essay_labeled_sentences.append((sent.strip(), 1))\n",
    "        else:\n",
    "            essay_labeled_sentences.append((sent.strip(), 0))\n",
    "        sent, sent_token_labels = '', []\n",
    "        \n",
    "        labeled_sentences.append(essay_labeled_sentences)\n",
    "        essay_labeled_sentences = []\n",
    "    \n",
    "    return labeled_sentences\n",
    "\n",
    "\n",
    "def essay_sent_list_to_merged_sent_list(file_sent_list, output_file, get_next_sent=False):\n",
    "        labeled_sent = []\n",
    "        \n",
    "        if not get_next_sent:\n",
    "            for file in file_sent_list:\n",
    "                for line in file:\n",
    "                    sent, label = line\n",
    "                    labeled_sent.append('{}\\t{}\\n'.format(sent, label))\n",
    "        \n",
    "        # sent with next\n",
    "        else:\n",
    "            for file in file_sent_list:\n",
    "                for i, line in enumerate(file[:-1]):\n",
    "                    sent, label = line\n",
    "                    next_sent, _ =  file[i+1]\n",
    "                    labeled_sent.append('{}\\t{}\\t{}\\n'.format(sent, next_sent, label))\n",
    "                \n",
    "                sent, label = file[-1]\n",
    "                labeled_sent.append('{}\\t{}\\t{}\\n'.format(sent, 'END_OF_ESSAY', label))\n",
    "        \n",
    "        \n",
    "        with open(output_file,'w') as writer:\n",
    "            for line in labeled_sent:\n",
    "                writer.write(line)\n",
    "        \n",
    "        return labeled_sent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017_tok/train/*.tsv\")):\n",
    "    essay = open(file).readlines()\n",
    "    train_files.append(essay[1:])\n",
    "\n",
    "test_files = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017_tok/test/*.tsv\")):\n",
    "    essay = open(file).readlines()\n",
    "    test_files.append(essay[1:])\n",
    "\n",
    "\n",
    "train_sent = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(train_files),\n",
    "                                                '../data/SG2017_tok/sentences/train.tsv')\n",
    "test_sent = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(test_files),\n",
    "                                               '../data/SG2017_tok/sentences/test.tsv')\n",
    "\n",
    "train_sent_next = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(train_files),\n",
    "                                                      '../data/SG2017_tok/sentences_next/train.tsv',\n",
    "                                                      get_next_sent=True)\n",
    "test_sent_next = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(test_files),\n",
    "                                                     '../data/SG2017_tok/sentences_next/test.tsv',\n",
    "                                                     get_next_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017_claim/train/*.tsv\")):\n",
    "    essay = open(file).readlines()\n",
    "    train_files.append(essay[1:])\n",
    "\n",
    "test_files = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017_claim/test/*.tsv\")):\n",
    "    essay = open(file).readlines()\n",
    "    test_files.append(essay[1:])\n",
    "\n",
    "\n",
    "train_sent = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(train_files),\n",
    "                                                '../data/SG2017_claim/sentences/train.tsv')\n",
    "test_sent = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(test_files),\n",
    "                                               '../data/SG2017_claim/sentences/test.tsv')\n",
    "\n",
    "train_sent_next = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(train_files),\n",
    "                                                      '../data/SG2017_claim/sentences_next/train.tsv',\n",
    "                                                      get_next_sent=True)\n",
    "test_sent_next = essay_sent_list_to_merged_sent_list(essay_list_token_to_essay_list_sent(test_files),\n",
    "                                                     '../data/SG2017_claim/sentences_next/test.tsv',\n",
    "                                                     get_next_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
