{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import networkx as nx\n",
    "model_dir = '/Users/talhindi/miniconda3/lib/python3.7/site-packages/en_core_web_sm/en_core_web_sm-2.1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = pd.read_csv('../data/SG2017/train-test-split.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_txt_prg_list = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017/*.txt\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_txt_prg_list.append(essay)\n",
    "\n",
    "essay_txt_str = []\n",
    "for essay in essays_txt_prg_list:\n",
    "    essay_txt_str.append(''.join(essay))\n",
    "    \n",
    "essays_ann = []\n",
    "for file in sorted(glob.glob(\"../data/SG2017/*.ann\")):\n",
    "    essay = open(file).readlines()\n",
    "    essays_ann.append(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays_segments = []\n",
    "\n",
    "for essay in essays_ann:    \n",
    "    segments = []\n",
    "    \n",
    "    for line in essay:\n",
    "        if line[0] == 'T':\n",
    "            _, label_s_e, text = line.rstrip().split('\\t')\n",
    "            label, start, end = label_s_e.split()\n",
    "            segments.append((label, int(start), int(end), text))\n",
    "            \n",
    "    segments.sort(key = lambda element : element[1])\n",
    "    essays_segments.append(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(essay_spacy, segments):\n",
    "    '''O = 0, Arg-B = 1, Arg-I = 2'''\n",
    "    \n",
    "    doc_len = len(essay_spacy)\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "    arg_seg_starts = [start for arg_type, start, end, text in segments]\n",
    "    \n",
    "    for token in essay_spacy:\n",
    "        arg_I_token = False\n",
    "\n",
    "        if token.idx in arg_seg_starts:\n",
    "            labels.append('Arg-B')\n",
    "#             labels.append(1.0)\n",
    "            tokens.append(token.text)\n",
    "            assert token.text in segments[arg_seg_starts.index(token.idx)][-1]\n",
    "        else:\n",
    "            for _, start, end, _ in segments:\n",
    "                if token.idx > start and token.idx+len(token) <= end:\n",
    "                    labels.append('Arg-I')\n",
    "#                     labels.append(2.0)\n",
    "                    tokens.append(token.text)\n",
    "                    arg_I_token = True\n",
    "            if not arg_I_token:\n",
    "                labels.append('O')\n",
    "#                 labels.append(0.0)\n",
    "                tokens.append(token.text)\n",
    "\n",
    "    assert len(labels) == doc_len\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Claim', 'MajorClaim', 'Premise'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([arg_type for arg_type, start, end, text in essays_segments[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(model_dir)\n",
    "\n",
    "essay_spacy = []\n",
    "for essay in essay_txt_str:\n",
    "    essay_spacy.append(nlp(essay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_prg_spacy = []\n",
    "for essay_prgs in essays_txt_prg_list:\n",
    "    prg_list_tok = []\n",
    "    for prg in essay_prgs:\n",
    "        prg_list_tok.append(nlp(prg))\n",
    "\n",
    "    essay_prg_spacy.append(prg_list_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(defaultdict(int, {'O': 39617, 'Arg-B': 4823, 'Arg-I': 75312}),\n",
       " defaultdict(int, {'O': 9801, 'Arg-B': 1266, 'Arg-I': 18748}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting labels from each type\n",
    "# without new lines\n",
    "token_labels = []\n",
    "train_BIO = defaultdict(int)\n",
    "test_BIO = defaultdict(int)\n",
    "\n",
    "for doc, segments, group in zip(essay_spacy, essays_segments, train_test_split.SET):\n",
    "    tokens, labels = get_labels(doc, segments)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        for label in  labels:\n",
    "            train_BIO[label] += 1\n",
    "    else:\n",
    "        for label in  labels:\n",
    "            test_BIO[label] += 1\n",
    "\n",
    "\n",
    "train_BIO,test_BIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Tokenized and Labeled Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim + premise merged\n",
    "train_file  = open('../data/SG2017_tok/train.txt', 'w')\n",
    "test_file  = open('../data/SG2017_tok/test.txt', 'w')\n",
    "\n",
    "# train_dep_file  = open('../data/SG2017_tokenized/dep/train.txt', 'w')\n",
    "# test_dep_file  = open('../data/SG2017_tokenized/dep/test.txt', 'w')\n",
    "\n",
    "for essay_id, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "    tokens, labels = get_labels(doc, segments)\n",
    "    labeled_token_id = 0\n",
    "    \n",
    "    if essay_id+1 < 10:\n",
    "        essay_3digit_id = '00'+str(essay_id+1)\n",
    "    elif essay_id+1 < 100:\n",
    "        essay_3digit_id = '0'+str(essay_id+1)\n",
    "    else:\n",
    "        essay_3digit_id = str(essay_id+1)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../data/SG2017_tok/train/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        train_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "#                         train_dep_file.write('{}_{} {}\\n'.format(token.text, token.dep_, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                train_file.write('\\n')\n",
    "#                 train_dep_file.write('\\n')\n",
    "                \n",
    "    else:\n",
    "        with open('../data/SG2017_tok/test/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        test_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "#                         test_dep_file.write('{}_{} {}\\n'.format(token.text, token.dep_, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                test_file.write('\\n')  \n",
    "#                 test_dep_file.write('\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim + premise merged with dep relation\n",
    "train_dep_file  = open('../data/SG2017_tok_dep/train.txt', 'w')\n",
    "test_dep_file  = open('../data/SG2017_tok_dep/test.txt', 'w')\n",
    "\n",
    "for essay_id, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "    tokens, labels = get_labels(doc, segments)\n",
    "    labeled_token_id = 0\n",
    "    \n",
    "    if essay_id+1 < 10:\n",
    "        essay_3digit_id = '00'+str(essay_id+1)\n",
    "    elif essay_id+1 < 100:\n",
    "        essay_3digit_id = '0'+str(essay_id+1)\n",
    "    else:\n",
    "        essay_3digit_id = str(essay_id+1)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../data/SG2017_tok_dep/train/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        train_dep_file.write('{}_{} {}\\n'.format(token.text, token.dep_, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                train_dep_file.write('\\n')\n",
    "                \n",
    "    else:\n",
    "        with open('../data/SG2017_tok_dep/test/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        test_dep_file.write('{}_{} {}\\n'.format(token.text, token.dep_, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                test_dep_file.write('\\n')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperating Claim and Premise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_claim_premise(essay_spacy, segments, mode='claim'):\n",
    "    '''labels are: B-claim, I-claim, B-premise, I-premise, O'''\n",
    "    '''modes are: claim, premise, all'''\n",
    "    \n",
    "    assert mode in ['claim', 'premise', 'all']\n",
    "    \n",
    "    mode_labels= {'claim':['Claim','MajorClaim'],\n",
    "                  'premise':'Premise',\n",
    "                  'all': ['Claim', 'MajorClaim', 'Premise']}\n",
    "    \n",
    "    arg_type_to_tag = {'MajorClaim': 'claim',\n",
    "                      'Claim': 'claim',\n",
    "                      'Premise':'premise'}\n",
    "    \n",
    "    doc_len = len(essay_spacy)\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    arg_seg_starts = [start for arg_type, start, end, text in segments if arg_type in mode_labels[mode]]\n",
    "    arg_seg_arg_type = [arg_type for arg_type, start, end, text in segments if arg_type in mode_labels[mode]]\n",
    "    arg_seg_texts = [text for arg_type, start, end, text in segments if arg_type in mode_labels[mode]]\n",
    "    \n",
    "    for token in essay_spacy:\n",
    "        arg_I_token = False\n",
    "\n",
    "        if token.idx in arg_seg_starts:\n",
    "            labels.append('B-' + arg_type_to_tag[arg_seg_arg_type[arg_seg_starts.index(token.idx)]])\n",
    "            tokens.append(token.text)\n",
    "            assert token.text in arg_seg_texts[arg_seg_starts.index(token.idx)]\n",
    "        \n",
    "        else:\n",
    "            for arg_type, start, end, _ in segments:\n",
    "                if arg_type in mode_labels[mode]:\n",
    "                    if token.idx > start and token.idx+len(token) <= end:\n",
    "                        labels.append('I-'+arg_type_to_tag[arg_type])\n",
    "                        tokens.append(token.text)\n",
    "                        arg_I_token = True\n",
    "            \n",
    "            if not arg_I_token:\n",
    "                if mode == 'claim':\n",
    "                    labels.append('O-claim')\n",
    "                elif mode == 'premise':\n",
    "                    labels.append('O-premise')\n",
    "                else:\n",
    "                    labels.append('O')\n",
    "                tokens.append(token.text)\n",
    "\n",
    "    \n",
    "    assert len(labels) == doc_len\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exporting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim only\n",
    "train_file  = open('../data/SG2017_claim/train.txt', 'w')\n",
    "test_file  = open('../data/SG2017_claim/test.txt', 'w')\n",
    "\n",
    "\n",
    "for essay_id, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "    tokens, labels = get_labels_claim_premise(doc, segments, mode='claim')\n",
    "    labeled_token_id = 0\n",
    "    \n",
    "    if essay_id+1 < 10:\n",
    "        essay_3digit_id = '00'+str(essay_id+1)\n",
    "    elif essay_id+1 < 100:\n",
    "        essay_3digit_id = '0'+str(essay_id+1)\n",
    "    else:\n",
    "        essay_3digit_id = str(essay_id+1)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../data/SG2017_claim/train/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        train_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                train_file.write('\\n')\n",
    "                \n",
    "    else:\n",
    "        with open('../data/SG2017_claim/test/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        test_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                test_file.write('\\n')  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# premise only\n",
    "train_file  = open('../data/SG2017_premise/train.txt', 'w')\n",
    "test_file  = open('../data/SG2017_premise/test.txt', 'w')\n",
    "\n",
    "\n",
    "for essay_id, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "    tokens, labels = get_labels_claim_premise(doc, segments, mode='premise')\n",
    "    labeled_token_id = 0\n",
    "    \n",
    "    if essay_id+1 < 10:\n",
    "        essay_3digit_id = '00'+str(essay_id+1)\n",
    "    elif essay_id+1 < 100:\n",
    "        essay_3digit_id = '0'+str(essay_id+1)\n",
    "    else:\n",
    "        essay_3digit_id = str(essay_id+1)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../data/SG2017_premise/train/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        train_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                train_file.write('\\n')\n",
    "                \n",
    "    else:\n",
    "        with open('../data/SG2017_premise/test/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        test_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                test_file.write('\\n')  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim and premise separated\n",
    "train_file  = open('../data/SG2017_claim_premise/train.txt', 'w')\n",
    "test_file  = open('../data/SG2017_claim_premise/test.txt', 'w')\n",
    "\n",
    "\n",
    "for essay_id, (doc, segments, group) in enumerate(zip(essay_spacy, essays_segments, train_test_split.SET)):\n",
    "    \n",
    "    tokens, labels = get_labels_claim_premise(doc, segments, mode='all')\n",
    "    labeled_token_id = 0\n",
    "    \n",
    "    if essay_id+1 < 10:\n",
    "        essay_3digit_id = '00'+str(essay_id+1)\n",
    "    elif essay_id+1 < 100:\n",
    "        essay_3digit_id = '0'+str(essay_id+1)\n",
    "    else:\n",
    "        essay_3digit_id = str(essay_id+1)\n",
    "    \n",
    "    if group == \"TRAIN\":\n",
    "        with open('../data/SG2017_claim_premise/train/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        train_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                train_file.write('\\n')\n",
    "                \n",
    "    else:\n",
    "        with open('../data/SG2017_claim_premise/test/essay{}.tsv'.format(essay_3digit_id), 'w') as file:\n",
    "            file.write('sentence_id\\ttoken_id\\ttoken\\tlabel\\n')\n",
    "            for sent_id, sent in enumerate(doc.sents):\n",
    "                for token_id, token in enumerate(sent):\n",
    "                    assert token.text == tokens[labeled_token_id]\n",
    "                    file.write('{}\\t{}\\t{}\\t{}\\n'.format(sent_id, token_id, token.text.replace('\\n','_NEW_LINE_'), labels[labeled_token_id]))\n",
    "                    \n",
    "                    if '\\n' not in token.text:\n",
    "                        test_file.write('{} {}\\n'.format(token.text, labels[labeled_token_id]))\n",
    "                    \n",
    "                    labeled_token_id += 1\n",
    "                    \n",
    "                test_file.write('\\n')  \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
